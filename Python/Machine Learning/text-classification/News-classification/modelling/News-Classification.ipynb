{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "## So far we have preprocessed the given news articles and corresponding labels and made them ready to perform modelling on them. In this notebook we will be performing modelling on the news articles using the following algorithms:\n",
    "### - Naive bayes\n",
    "### - Logistic regression\n",
    "### - Linear SVM\n",
    "### - Multilayer perceptron\n",
    "### - LSTM - with ngram collocation and using pretrained wordembedding\n",
    "### - CNN - - with ngram collocation and using pretrained wordembedding\n",
    "### - CNN + LSTM -  with ngram collocation and using pretrained wordembedding\n",
    "### - FastText \n",
    "======================================================================================================================"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,GlobalAveragePooling1D\n",
    "from keras.callbacks import  EarlyStopping\n",
    "import re\n",
    "from fastText import train_supervised"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filepath):\n",
    "    temp_data = open(filepath,'r')\n",
    "    temp_data = temp_data.readlines()\n",
    "    temp_data = [x.replace('\\n','') for x in  temp_data]\n",
    "    return temp_data\n",
    "\n",
    "def read_label(filepath):\n",
    "    temp_pd  = pd.read_csv(filepath)\n",
    "    temp_pd = temp_pd.y.str.replace('C','')\n",
    "    return list(temp_pd.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VM\\Anaconda3\\lib\\site-packages\\ipykernel\\__main__.py:12: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    }
   ],
   "source": [
    "train_data_ngram = read_data('../Data/processed/train_data1.txt')\n",
    "train_data_no_ngram = read_data('../Data/processed/train_data_no_ngram.txt')\n",
    "val_data_ngram = read_data('../Data/processed/validation_data1.txt')\n",
    "val_data_no_ngram = read_data('../Data/processed/validation_data_no_ngram.txt')\n",
    "complete_data_ngram = read_data('../Data/processed/complete_data1.txt')\n",
    "complete_data_no_ngram = read_data ('../Data/processed/complete_data_no_ngram.txt')\n",
    "train_label_ngram = read_label('../Data/processed/train_labels1.csv')\n",
    "train_label_no_ngram = read_label('../Data/processed/train_labels_no_ngram.csv')\n",
    "val_label_ngram = read_label('../Data/processed/validation_labels1.csv')\n",
    "val_label_no_ngram  = read_label('../Data/processed/validation_labels_no_ngram.csv')\n",
    "test_labels = open('../Data/original/testing_labels.txt','r').readlines()\n",
    "test_labels = pd.DataFrame(test_labels).loc[:,0].str.extract('.+C(\\d+)\\n')\n",
    "test_data_ngram = read_data('../Data/processed/test_data1.txt')\n",
    "test_data_no_ngram = read_data('../Data/processed/test_data_no_ngram.txt')\n",
    "complete_labels_ngram =  train_label_ngram + val_label_ngram\n",
    "complete_labels_no_ngram = train_label_no_ngram + val_label_no_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary to store different model performances\n",
    "df_performances = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF IDF\n",
    "### First we will create models using TF IDF form of the data and evaluate their performance on validation data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For ngram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning the tfidf vectorizer to learn vocabulary and the id values of each word from training data\n",
    "tfidf_vectorizer = TfidfVectorizer(input = 'content', analyzer = 'word')\n",
    "tfidf_train_data_ngram = tfidf_vectorizer.fit_transform(complete_data_ngram)\n",
    "# transforming test data into idf vector\n",
    "tfidf_test_data_ngram = tfidf_vectorizer.transform(test_data_ngram) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For data without ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning the tfidf vectorizer to learn vocabulary and the id values of each word from training data\n",
    "tfidf_vectorizer = TfidfVectorizer(input = 'content', analyzer = 'word')\n",
    "tfidf_train_data_no_ngram = tfidf_vectorizer.fit_transform(complete_data_no_ngram)\n",
    "# transforming test data into idf vector\n",
    "tfidf_test_data_no_ngram = tfidf_vectorizer.transform(test_data_no_ngram) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier\n",
    "=================================\n",
    "### 5 fold Crossvalidation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a stratified shuffled split\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_MNB = MultinomialNB(alpha = 0.1)\n",
    "crossval_arr = cross_val_score(clf_MNB, tfidf_train_data_ngram, complete_labels_ngram, cv=sss)\n",
    "avg_nb = sum(crossval_arr)/len(crossval_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB 5 fold crossvalidation avg accuracy =   0.7528019164826907\n"
     ]
    }
   ],
   "source": [
    "print ('NB 5 fold crossvalidation avg accuracy =  ',avg_nb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_MNB = clf_MNB.fit(tfidf_train_data_ngram, complete_labels_ngram)\n",
    "predicted_nb = clf_MNB.predict(tfidf_test_data_ngram)\n",
    "nb_accu = accuracy_score (test_labels,predicted_nb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7399098083427283"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to performance dictionary\n",
    "df_performances['NB_ngram'] = nb_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classifier\n",
    "=======================\n",
    "### 5 fold Crossvalidation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm= OneVsOneClassifier(SGDClassifier(loss='hinge', penalty='l2',alpha=1e-4, max_iter=50, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM 5 fold crossvalidation avg accuracy =   0.781708863732444\n"
     ]
    }
   ],
   "source": [
    "crossval_arr_svm = cross_val_score(clf_svm, tfidf_train_data_ngram, complete_labels_ngram, cv=sss)\n",
    "avg_svm = sum(crossval_arr_svm)/len(crossval_arr_svm)\n",
    "print ('SVM 5 fold crossvalidation avg accuracy =  ',avg_svm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7685080796692972"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_svm = clf_svm.fit(tfidf_train_data_ngram, complete_labels_ngram)\n",
    "predicted_svm = clf_svm.predict(tfidf_test_data_ngram)\n",
    "svm_accu = accuracy_score (test_labels,predicted_svm)\n",
    "svm_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to performance dictionary\n",
    "df_performances['SVM_ngram'] = svm_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106443, 45367)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_data_ngram.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier\n",
    "========================\n",
    "### 2 Crossvalidation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VM\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\VM\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 neuron MLP 2 fold crossvalidation avg accuracy =   0.7817417445629198\n"
     ]
    }
   ],
   "source": [
    "# Creating a stratified shuffled split\n",
    "sss_mlp = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=321)\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,), learning_rate_init=0.001, max_iter=2)\n",
    "crossval_arr_mlp = cross_val_score(mlp_clf, tfidf_train_data_ngram, complete_labels_ngram, cv=sss_mlp)\n",
    "avg_mlp = sum(crossval_arr_mlp)/len(crossval_arr_mlp)\n",
    "print ('100 neuron MLP 2 fold crossvalidation avg accuracy =  ',avg_mlp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VM\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7719654265313792"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp = mlp_clf.fit(tfidf_train_data_ngram, complete_labels_ngram)\n",
    "predicted_mlp = mlp.predict(tfidf_test_data_ngram)\n",
    "mlp_accu = accuracy_score (test_labels,predicted_mlp)\n",
    "mlp_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to performance dictionary\n",
    "df_performances['MLP_100N_ngram'] = mlp_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "=========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 5 fold crossvalidation avg accuracy =   0.7802808962374935\n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "crossval_arr = cross_val_score(lr_clf, tfidf_train_data_ngram, complete_labels_ngram, cv=sss)\n",
    "avg_lr = sum(crossval_arr)/len(crossval_arr)\n",
    "print ('LR 5 fold crossvalidation avg accuracy =  ',avg_lr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7676061630965803"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_clf = lr_clf.fit(tfidf_train_data_ngram, complete_labels_ngram)\n",
    "predicted_lr = lr_clf.predict(tfidf_test_data_ngram)\n",
    "lr_accu = accuracy_score (test_labels,predicted_lr)\n",
    "lr_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to performance dictionary\n",
    "df_performances['LR_ngram'] = lr_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe\n",
    "### GloVe is a type of model which after getting trained can be used to create word embedding i.e. words representation in a vector space.  We will use pretrained GloVe model which has been trained on news as well as wikipedia which is a corpus similar to our own. Since these vector representation can retain some sort of semantic information, using it may yield better performance.\n",
    "### For deep learning techniques, we intend to use transfer learning by using pretrained word embedding in our model and to let it learn from our corpus as well to save time. And finally these word embeddings will be used as input features to various models. \n",
    "### The pretrained wordembedding file name is glove.6B.100d, glove.6B.200d, glove.6B.300d and can be downloaded from :\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Glove word embedding in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGlove(Glove_filepath):\n",
    "    embeddings_index = {}\n",
    "    f = open(Glove_filepath, encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0] ## The first entry is the word\n",
    "        coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('GloVe data loaded')\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe data loaded\n"
     ]
    }
   ],
   "source": [
    "glove_dict = loadGlove('glove.6B.100d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "### We will first be using data without ngrams for CNN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 116063 unique words.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "# Getting word embedding \n",
    "MAX_SEQUENCE_LENGTH = 130\n",
    "\n",
    "tokenizer1 = Tokenizer()\n",
    "tokenizer1.fit_on_texts(complete_data_no_ngram)\n",
    "\n",
    "complete_train_no_ngram = tokenizer1.texts_to_sequences(complete_data_no_ngram)\n",
    "complete_test_no_ngram = tokenizer1.texts_to_sequences(test_data_no_ngram)\n",
    "\n",
    "word_index_no_ngram = tokenizer1.word_index\n",
    "vocab_size = len(word_index_no_ngram)\n",
    "max_words = vocab_size\n",
    "\n",
    "\n",
    "print('Found %s unique words.' % len(word_index_no_ngram))\n",
    "\n",
    "#padding the sequences\n",
    "complete_train_no_ngram_padded = pad_sequences(complete_train_no_ngram, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "complete_test_no_ngram_padded = pad_sequences(complete_test_no_ngram, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "complete_labels_no_ngram = np.array(list(map(int, complete_labels_no_ngram)))\n",
    "complete_labels_no_ngram = complete_labels_no_ngram - 1\n",
    "\n",
    "test_labels = np.array(list(map(int, test_labels)))\n",
    "test_labels = test_labels - 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "\n",
    "# Preparing embedding matrix \n",
    "EMBEDDING_DIM = glove_dict.get('a').shape[0]\n",
    "num_words = max_words + 1\n",
    "embedding_matrix_no_ngram = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index_no_ngram.items():\n",
    "    if i>max_words:\n",
    "        continue\n",
    "    embedding_vector = glove_dict.get(word)\n",
    "    if(embedding_vector is not None):\n",
    "        embedding_matrix_no_ngram[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix_no_ngram),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True) # We set trainable to True because it gives better performance by optimizing the weights more\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does an embedding matrix look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105185, 100)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXYAAAD8CAYAAABjAo9vAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/Bf6xmHeDKpi8wAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(embedding_matrix_no_ngram.shape)\n",
    "plt.plot(embedding_matrix_no_ngram[16])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(GlobalAveragePooling1D())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Conv1D(64, 5, activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=4))\n",
    "#model.add(LSTM(100))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(23, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "85154"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_train_no_ngram_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 101120 samples, validate on 5323 samples\n",
      "Epoch 1/7\n",
      "101120/101120 [==============================] - 107s 1ms/step - loss: 1.1855 - acc: 0.6783 - val_loss: 0.8698 - val_acc: 0.7468\n",
      "Epoch 2/7\n",
      "101120/101120 [==============================] - 98s 967us/step - loss: 0.7512 - acc: 0.7749 - val_loss: 0.7733 - val_acc: 0.7684\n",
      "Epoch 3/7\n",
      "101120/101120 [==============================] - 96s 945us/step - loss: 0.6199 - acc: 0.8079 - val_loss: 0.7369 - val_acc: 0.7796\n",
      "Epoch 4/7\n",
      "101120/101120 [==============================] - 96s 950us/step - loss: 0.5176 - acc: 0.8365 - val_loss: 0.7450 - val_acc: 0.7796\n",
      "Epoch 5/7\n",
      "101120/101120 [==============================] - 96s 946us/step - loss: 0.4275 - acc: 0.8646 - val_loss: 0.7556 - val_acc: 0.7774\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2112a64fda0>"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping_callback = EarlyStopping(monitor='val_loss', patience=2,restore_best_weights=True)\n",
    "lr_reduce_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)        \n",
    "model.fit(complete_train_no_ngram_padded,complete_labels_no_ngram\n",
    "          ,batch_size = 32, validation_split=0.05, epochs = 7, callbacks = [earlystopping_callback,lr_reduce_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7627207816610296"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_ngram_cnn_pred7 = model.predict(complete_test_no_ngram_padded)\n",
    "no_ngram_cnn_pred7 = list(map(np.argmax, no_ngram_cnn_pred7))\n",
    "no_ngram_cnn_accu7 = accuracy_score (test_labels,no_ngram_cnn_pred7)\n",
    "no_ngram_cnn_accu7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now training on complete data to find accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "106443/106443 [==============================] - 69s 644us/step - loss: 0.6590 - acc: 0.8397\n",
      "Epoch 2/3\n",
      "106443/106443 [==============================] - 64s 599us/step - loss: 0.2596 - acc: 0.9302\n",
      "Epoch 3/3\n",
      "106443/106443 [==============================] - 70s 662us/step - loss: 0.1919 - acc: 0.9481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2112d2335f8>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(embedding_layer)\n",
    "model2.add(GlobalAveragePooling1D())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Conv1D(64, 5, activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=4))\n",
    "#model.add(LSTM(100))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dense(23, activation='softmax'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#earlystopping_callback = EarlyStopping(monitor='val_loss', patience=2,restore_best_weights=True)\n",
    "#lr_reduce_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)        \n",
    "model2.fit(complete_train_no_ngram_padded,complete_labels_no_ngram\n",
    "          ,batch_size = 32, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7218338970311913"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_ngram_cnn_pred = model2.predict(complete_test_no_ngram_padded)\n",
    "no_ngram_cnn_pred1 = list(map(np.argmax, no_ngram_cnn_pred))\n",
    "no_ngram_cnn_accu = accuracy_score (test_labels,no_ngram_cnn_pred1)\n",
    "no_ngram_cnn_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now using data with ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40747 unique words.\n"
     ]
    }
   ],
   "source": [
    "#complete_data_ngram\n",
    "#complete_labels_ngram\n",
    "\n",
    "\n",
    "# Getting word embedding \n",
    "MAX_SEQUENCE_LENGTH = 130\n",
    "\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(complete_data_ngram)\n",
    "\n",
    "complete_train_ngram = tokenizer2.texts_to_sequences(complete_data_ngram)\n",
    "complete_test_ngram = tokenizer2.texts_to_sequences(test_data_ngram)\n",
    "\n",
    "word_index_ngram = tokenizer2.word_index\n",
    "vocab_size = len(word_index_ngram)\n",
    "max_words = vocab_size\n",
    "\n",
    "\n",
    "print('Found %s unique words.' % len(word_index_ngram))\n",
    "\n",
    "#padding the sequences\n",
    "complete_train_ngram_padded = pad_sequences(complete_train_ngram, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "complete_test_ngram_padded = pad_sequences(complete_test_ngram, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "complete_labels_ngram = np.array(list(map(int, complete_labels_ngram)))\n",
    "complete_labels_ngram = complete_labels_ngram - 1\n",
    "\n",
    "#test_labels = np.array(list(map(int, test_labels)))\n",
    "#test_labels = test_labels - 1\n",
    "\n",
    "# Preparing embedding matrix \n",
    "EMBEDDING_DIM = glove_dict.get('a').shape[0]\n",
    "num_words = max_words + 1\n",
    "embedding_matrix_ngram = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index_ngram.items():\n",
    "    if i>max_words:\n",
    "        continue\n",
    "    embedding_vector = glove_dict.get(word)\n",
    "    if(embedding_vector is not None):\n",
    "        embedding_matrix_ngram[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix_ngram),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True) # We set trainable to True because it gives better performance by optimizing the weights more\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ngram = Sequential()\n",
    "model_ngram.add(embedding_layer)\n",
    "model_ngram.add(GlobalAveragePooling1D())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Conv1D(64, 5, activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=4))\n",
    "#model.add(LSTM(100))\n",
    "model_ngram.add(Dense(64, activation='relu'))\n",
    "model_ngram.add(Dense(23, activation='softmax'))\n",
    "model_ngram.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 101120 samples, validate on 5323 samples\n",
      "Epoch 1/7\n",
      "101120/101120 [==============================] - 68s 674us/step - loss: 1.1924 - acc: 0.6772 - val_loss: 0.9022 - val_acc: 0.7338\n",
      "Epoch 2/7\n",
      "101120/101120 [==============================] - 60s 594us/step - loss: 0.7680 - acc: 0.7722 - val_loss: 0.8066 - val_acc: 0.7509\n",
      "Epoch 3/7\n",
      "101120/101120 [==============================] - 63s 627us/step - loss: 0.6437 - acc: 0.8017 - val_loss: 0.7817 - val_acc: 0.7642\n",
      "Epoch 4/7\n",
      "101120/101120 [==============================] - 63s 624us/step - loss: 0.5493 - acc: 0.8285 - val_loss: 0.7850 - val_acc: 0.7618\n",
      "Epoch 5/7\n",
      "101120/101120 [==============================] - 63s 621us/step - loss: 0.4684 - acc: 0.8526 - val_loss: 0.8012 - val_acc: 0.7659\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x211294f31d0>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "earlystopping_callback = EarlyStopping(monitor='val_loss', patience=2,restore_best_weights=True)\n",
    "\n",
    "model_ngram.fit(complete_train_ngram_padded,complete_labels_ngram\n",
    "          ,batch_size = 32, validation_split=0.05, epochs = 7, callbacks = [earlystopping_callback,lr_reduce_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6874859075535513"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_cnn_pred7 = model_ngram.predict(complete_test_ngram_padded)\n",
    "ngram_cnn_pred7 = list(map(np.argmax, ngram_cnn_pred7))\n",
    "ngram_cnn_accu7 = accuracy_score (test_labels,ngram_cnn_pred7)\n",
    "ngram_cnn_accu7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "106443/106443 [==============================] - 66s 623us/step - loss: 0.9290 - acc: 0.7546\n",
      "Epoch 2/3\n",
      "106443/106443 [==============================] - 56s 523us/step - loss: 0.6166 - acc: 0.8153\n",
      "Epoch 3/3\n",
      "106443/106443 [==============================] - 57s 531us/step - loss: 0.5238 - acc: 0.8389\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2112d27c0b8>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ngram2 = Sequential()\n",
    "model_ngram2.add(embedding_layer)\n",
    "model_ngram2.add(GlobalAveragePooling1D())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Conv1D(64, 5, activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=4))\n",
    "#model.add(LSTM(100))\n",
    "model_ngram2.add(Dense(64, activation='relu'))\n",
    "model_ngram2.add(Dense(23, activation='softmax'))\n",
    "model_ngram2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_ngram2.fit(complete_train_ngram_padded,complete_labels_ngram\n",
    "          ,batch_size = 32, epochs = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7585494175122135"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_cnn_pred = model_ngram2.predict(complete_test_ngram_padded)\n",
    "ngram_cnn_pred1 = list(map(np.argmax, ngram_cnn_pred))\n",
    "ngram_cnn_accu = accuracy_score (test_labels,ngram_cnn_pred1)\n",
    "ngram_cnn_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "106443/106443 [==============================] - 69s 650us/step - loss: 0.8009 - acc: 0.7936\n",
      "Epoch 2/5\n",
      "106443/106443 [==============================] - 63s 596us/step - loss: 0.5013 - acc: 0.8505\n",
      "Epoch 3/5\n",
      "106443/106443 [==============================] - 65s 608us/step - loss: 0.4220 - acc: 0.8721\n",
      "Epoch 4/5\n",
      "106443/106443 [==============================] - 65s 612us/step - loss: 0.3591 - acc: 0.8896\n",
      "Epoch 5/5\n",
      "106443/106443 [==============================] - 64s 602us/step - loss: 0.3062 - acc: 0.9062\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2112d22ce80>"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ngram5 = Sequential()\n",
    "model_ngram5.add(embedding_layer)\n",
    "model_ngram5.add(GlobalAveragePooling1D())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Conv1D(64, 5, activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=4))\n",
    "#model.add(LSTM(100))\n",
    "model_ngram5.add(Dense(64, activation='relu'))\n",
    "model_ngram5.add(Dense(23, activation='softmax'))\n",
    "model_ngram5.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_ngram5.fit(complete_train_ngram_padded,complete_labels_ngram\n",
    "          ,batch_size = 32, epochs = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7447576099210823"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_cnn_pred5 = model_ngram5.predict(complete_test_ngram_padded)\n",
    "ngram_cnn_pred5 = list(map(np.argmax, ngram_cnn_pred5))\n",
    "ngram_cnn_accu5 = accuracy_score (test_labels,ngram_cnn_pred5)\n",
    "ngram_cnn_accu5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "model_lstm_cnn <- keras_model_sequential()\n",
    "\n",
    "model_lstm_cnn %>% \n",
    "    layer_embedding(input_dim = vocab_size, input_length = maxlen,output_dim = embedding_size) %>%\n",
    "    layer_dropout(0.25) %>%\n",
    "    layer_conv_1d(\n",
    "       filters, \n",
    "       kernel_size, \n",
    "       padding = \"valid\",\n",
    "       activation = \"relu\",\n",
    "       strides = 1\n",
    "    ) %>%\n",
    "  layer_max_pooling_1d(pool_size) %>%\n",
    "  layer_lstm(units= maxlen+50)%>%\n",
    "  layer_dense(units = 23, activation = \"softmax\")\n",
    "\n",
    "\n",
    "get_layer(model_lstm_cnn, index = 1) %>% \n",
    "  set_weights(list(embedding_matrix)) #%>% \n",
    "  #freeze_weights()\n",
    "\n",
    "model_lstm_cnn %>% summary()\n",
    "# len\n",
    "model_lstm_cnn %>% compile(\n",
    "  loss = \"sparse_categorical_crossentropy\",\n",
    "  optimizer = \"adam\",\n",
    "  metrics = \"accuracy\"\n",
    ")\n",
    "\n",
    "history_lstm_cnn <- model_lstm_cnn %>% fit(\n",
    "  train_data_ass,\n",
    "  complete_labels_for_cnn_n_lstm,\n",
    "  epochs =3,\n",
    "  batch_size = 32,\n",
    "  #validation_split=0.3,\n",
    "  validation_data = list(val_data_ass, y_val4),\n",
    "  verbose=1,\n",
    "  callbacks = list(\n",
    "  callback_model_checkpoint(\"checkpoints_cnn_lstm_zongz.h5\", monitor = \"val_acc\", verbose = 0,\n",
    "  save_best_only = TRUE, save_weights_only = FALSE, mode = \"max\", period = 1),\n",
    "  callback_reduce_lr_on_plateau(monitor = \"val_loss\", factor = 0.1)\n",
    "  )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As we can see from above experiments.... the test accuracy varies a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try with CNN + LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "106443/106443 [==============================] - 685s 6ms/step - loss: 0.6083 - acc: 0.8129\n",
      "Epoch 2/3\n",
      "106443/106443 [==============================] - 669s 6ms/step - loss: 0.4329 - acc: 0.8634\n",
      "Epoch 3/3\n",
      "106443/106443 [==============================] - 668s 6ms/step - loss: 0.3733 - acc: 0.8811\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21170f2e940>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "\n",
    "model_ngram_cnnlstm = Sequential()\n",
    "model_ngram_cnnlstm.add(embedding_layer)\n",
    "model_ngram_cnnlstm.add(Dropout(0.2))\n",
    "model_ngram_cnnlstm.add(Conv1D(64, 5, activation='relu'))\n",
    "model_ngram_cnnlstm.add(MaxPooling1D(pool_size=4))\n",
    "model_ngram_cnnlstm.add(LSTM(180))\n",
    "model_ngram_cnnlstm.add(Dense(64, activation='relu'))\n",
    "model_ngram_cnnlstm.add(Dense(23, activation='softmax'))\n",
    "model_ngram_cnnlstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_ngram_cnnlstm.fit(complete_train_ngram_padded,complete_labels_ngram\n",
    "          ,batch_size = 32, epochs = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106443, 130)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_train_ngram_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7311161217587373"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_cnnlstm_pred = model_ngram_cnnlstm.predict(complete_test_ngram_padded)\n",
    "ngram_cnnlstm_pred = list(map(np.argmax, ngram_cnnlstm_pred))\n",
    "ngram_cnnlstm_accu = accuracy_score (test_labels,ngram_cnnlstm_pred)\n",
    "ngram_cnnlstm_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN+ LSTM with different configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 103249 samples, validate on 3194 samples\n",
      "Epoch 1/5\n",
      "103249/103249 [==============================] - 245s 2ms/step - loss: 0.5124 - acc: 0.8435 - val_loss: 0.4518 - val_acc: 0.8591\n",
      "Epoch 2/5\n",
      "103249/103249 [==============================] - 393s 4ms/step - loss: 0.3369 - acc: 0.8970 - val_loss: 0.4764 - val_acc: 0.8547\n",
      "Epoch 3/5\n",
      "103249/103249 [==============================] - 621s 6ms/step - loss: 0.2670 - acc: 0.9171 - val_loss: 0.4835 - val_acc: 0.8594\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21241780438>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "\n",
    "model_ngram_cnnlstm1 = Sequential()\n",
    "model_ngram_cnnlstm1.add(embedding_layer)\n",
    "#model_ngram_cnnlstm.add(Dropout(0.2))\n",
    "model_ngram_cnnlstm1.add(Conv1D(64, 5, activation='relu'))\n",
    "model_ngram_cnnlstm1.add(MaxPooling1D(pool_size=4))\n",
    "model_ngram_cnnlstm1.add(LSTM(150))\n",
    "model_ngram_cnnlstm1.add(Dense(23, activation='softmax'))\n",
    "model_ngram_cnnlstm1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_ngram_cnnlstm1.fit(complete_train_ngram_padded,complete_labels_ngram\n",
    "          ,batch_size = 32, validation_split=0.03, epochs = 5, callbacks = [earlystopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7282600526118"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_cnnlstm_pred1 = model_ngram_cnnlstm1.predict(complete_test_ngram_padded)\n",
    "ngram_cnnlstm_pred1 = list(map(np.argmax, ngram_cnnlstm_pred1))\n",
    "ngram_cnnlstm_accu1 = accuracy_score (test_labels,ngram_cnnlstm_pred1)\n",
    "ngram_cnnlstm_accu1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN+ LSTM without ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 103249 samples, validate on 3194 samples\n",
      "Epoch 1/5\n",
      "103249/103249 [==============================] - 548s 5ms/step - loss: 0.4627 - acc: 0.8591 - val_loss: 0.3006 - val_acc: 0.9020\n",
      "Epoch 2/5\n",
      "103249/103249 [==============================] - 561s 5ms/step - loss: 0.2704 - acc: 0.9160 - val_loss: 0.3117 - val_acc: 0.9029\n",
      "Epoch 3/5\n",
      "103249/103249 [==============================] - 609s 6ms/step - loss: 0.2061 - acc: 0.9370 - val_loss: 0.3235 - val_acc: 0.8998\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21242367320>"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "\n",
    "model_no_ngram_cnnlstm = Sequential()\n",
    "model_no_ngram_cnnlstm.add(embedding_layer)\n",
    "#model_ngram_cnnlstm.add(GlobalAveragePooling1D())\n",
    "#model_ngram_cnnlstm.add(Dropout(0.2))\n",
    "model_no_ngram_cnnlstm.add(Conv1D(64, 5, activation='relu'))\n",
    "model_no_ngram_cnnlstm.add(MaxPooling1D(pool_size=4))\n",
    "model_no_ngram_cnnlstm.add(LSTM(150))\n",
    "model_no_ngram_cnnlstm.add(Dense(23, activation='softmax'))\n",
    "model_no_ngram_cnnlstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_no_ngram_cnnlstm.fit(complete_train_no_ngram_padded,complete_labels_no_ngram\n",
    "          ,batch_size = 32, validation_split=0.03, epochs = 5, callbacks = [earlystopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7055993987222848"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_ngram_cnnlstm_pred = model_no_ngram_cnnlstm.predict(complete_test_no_ngram_padded)\n",
    "no_ngram_cnnlstm_pred = list(map(np.argmax, no_ngram_cnnlstm_pred))\n",
    "no_ngram_cnnlstm_accu = accuracy_score (test_labels,no_ngram_cnnlstm_pred)\n",
    "no_ngram_cnnlstm_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with dropout and another dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 103249 samples, validate on 3194 samples\n",
      "Epoch 1/3\n",
      "103249/103249 [==============================] - 582s 6ms/step - loss: 0.5048 - acc: 0.8437 - val_loss: 0.3575 - val_acc: 0.8860\n",
      "Epoch 2/3\n",
      "103249/103249 [==============================] - 612s 6ms/step - loss: 0.3158 - acc: 0.9005 - val_loss: 0.3441 - val_acc: 0.8929\n",
      "Epoch 3/3\n",
      "103249/103249 [==============================] - 658s 6ms/step - loss: 0.2722 - acc: 0.9147 - val_loss: 0.3464 - val_acc: 0.8914\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x21243199358>"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "\n",
    "model_no_ngram_cnnlstm2 = Sequential()\n",
    "model_no_ngram_cnnlstm2.add(embedding_layer)\n",
    "#model_ngram_cnnlstm.add(GlobalAveragePooling1D())\n",
    "model_no_ngram_cnnlstm2.add(Dropout(0.2))\n",
    "model_no_ngram_cnnlstm2.add(Conv1D(64, 5, activation='relu'))\n",
    "model_no_ngram_cnnlstm2.add(MaxPooling1D(pool_size=4))\n",
    "model_no_ngram_cnnlstm2.add(LSTM(150))\n",
    "model_no_ngram_cnnlstm2.add(Dense(64, activation='relu'))\n",
    "model_no_ngram_cnnlstm2.add(Dense(23, activation='softmax'))\n",
    "model_no_ngram_cnnlstm2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_no_ngram_cnnlstm2.fit(complete_train_no_ngram_padded,complete_labels_no_ngram\n",
    "          ,batch_size = 32, validation_split=0.03, epochs = 3, callbacks = [earlystopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7092446448703494"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_ngram_cnnlstm_pred2 = model_no_ngram_cnnlstm2.predict(complete_test_no_ngram_padded)\n",
    "no_ngram_cnnlstm_pred2 = list(map(np.argmax, no_ngram_cnnlstm_pred2))\n",
    "no_ngram_cnnlstm_accu2 = accuracy_score (test_labels,no_ngram_cnnlstm_pred2)\n",
    "no_ngram_cnnlstm_accu2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Just to observe if results vary if i change the validation split by increasing it to 10%. Even though the training set used to train the data will decrease but will it have any counter intuitive result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95798 samples, validate on 10645 samples\n",
      "Epoch 1/3\n",
      "95798/95798 [==============================] - 613s 6ms/step - loss: 0.4884 - acc: 0.8518 - val_loss: 0.3401 - val_acc: 0.8959\n",
      "Epoch 2/3\n",
      "95798/95798 [==============================] - 618s 6ms/step - loss: 0.2875 - acc: 0.9107 - val_loss: 0.3331 - val_acc: 0.8959\n",
      "Epoch 3/3\n",
      "95798/95798 [==============================] - 589s 6ms/step - loss: 0.2419 - acc: 0.9248 - val_loss: 0.3397 - val_acc: 0.8988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2126fe1db00>"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "\n",
    "model_no_ngram_cnnlstm3 = Sequential()\n",
    "model_no_ngram_cnnlstm3.add(embedding_layer)\n",
    "#model_ngram_cnnlstm.add(GlobalAveragePooling1D())\n",
    "model_no_ngram_cnnlstm3.add(Dropout(0.2))\n",
    "model_no_ngram_cnnlstm3.add(Conv1D(64, 5, activation='relu'))\n",
    "model_no_ngram_cnnlstm3.add(MaxPooling1D(pool_size=4))\n",
    "model_no_ngram_cnnlstm3.add(LSTM(150))\n",
    "model_no_ngram_cnnlstm3.add(Dense(64, activation='relu'))\n",
    "model_no_ngram_cnnlstm3.add(Dense(23, activation='softmax'))\n",
    "model_no_ngram_cnnlstm3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_no_ngram_cnnlstm3.fit(complete_train_no_ngram_padded,complete_labels_no_ngram\n",
    "          ,batch_size = 32, validation_split=0.1, epochs = 3, callbacks = [earlystopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7101841413002631"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_ngram_cnnlstm_pred3 = model_no_ngram_cnnlstm3.predict(complete_test_no_ngram_padded)\n",
    "no_ngram_cnnlstm_pred3 = list(map(np.argmax, no_ngram_cnnlstm_pred3))\n",
    "no_ngram_cnnlstm_accu3 = accuracy_score (test_labels,no_ngram_cnnlstm_pred3)\n",
    "no_ngram_cnnlstm_accu3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN+ LSTM Summary\n",
    "### From above tests we can notice that the best accuracy was in the first attempt with ngrams using the complete dataset as training data\n",
    "=========================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fast text model \n",
    "==========================\n",
    "### A model developed by Facebook which can be used for text classification\n",
    "#### It can be installed from https://github.com/facebookresearch/fastText/tree/master/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will transform the input into required format. Each document is required to be preceded by \\_\\_label\\_\\_[label] and a space.\n",
    " Example : \\_\\_label_\\_19 year_man ballarat hospital minor_injury crash tree maryborough crash midnight intersection timor road police_investigate cause collision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ngrams can be specified in fasttext api we will use data without ngrams initially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the complete data and labels and concatenating them as required by API\n",
    "# Adding __label__ to each string in labels list\n",
    "training_labels_fastext_no_ngram = ['__label__'+str(a)+' ' for a in complete_labels_no_ngram]\n",
    "traindata_with_labels_fastext_no_ngram = [\"{}{}\".format(labels_,docs_) for  labels_,docs_ in zip(training_labels_fastext_no_ngram,complete_data_no_ngram)]\n",
    "# Doing same for test data without ngram\n",
    "testing_labels_fastext = ['__label__'+str(a)+' ' for a in test_labels]\n",
    "testdata_with_labels_fastext_no_ngram = [\"{}{}\".format(labels_,docs_) for  labels_,docs_ in zip(testing_labels_fastext,test_data_no_ngram)]\n",
    "\n",
    "\n",
    "# we can also do the same with data with ngram just for testing purposes for later modelling\n",
    "training_labels_fastext_ngram = ['__label__'+str(a)+' ' for a in complete_labels_ngram]\n",
    "traindata_with_labels_fastext_ngram = [\"{}{}\".format(labels_,docs_) for  labels_,docs_ in zip(training_labels_fastext_ngram,complete_data_ngram)]\n",
    "\n",
    "# Doing same for test data with ngram\n",
    "testdata_with_labels_fastext_ngram = [\"{}{}\".format(labels_,docs_) for  labels_,docs_ in zip(testing_labels_fastext,test_data_ngram)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to disk since the input for fast text model accepts a filepath\n",
    "with open('traindata_with_labels_fastext_no_ngram.txt','w+') as o_fh:\n",
    "    for doc in traindata_with_labels_fastext_no_ngram:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()                \n",
    " \n",
    "    \n",
    "with open('testdata_with_labels_fastext_no_ngram.txt','w+') as o_fh:\n",
    "    for doc in testdata_with_labels_fastext_no_ngram:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()                \n",
    "\n",
    "with open('traindata_with_labels_fastext_ngram.txt','w+') as o_fh:\n",
    "    for doc in traindata_with_labels_fastext_ngram:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()                \n",
    "\n",
    "with open('testdata_with_labels_fastext_ngram.txt','w+') as o_fh:\n",
    "    for doc in testdata_with_labels_fastext_ngram:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# From previous testing we already have optimal parameters for this corpus\n",
    "# to make a model so we will directly be checking accuracy on test data\n",
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))\n",
    "model = train_supervised(input = 'traindata_with_labels_fastext_no_ngram.txt',lr=0.8,dim=50,epoch=25,minCount=1,wordNgrams=3,loss=\"softmax\",verbose=2,thread = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t26134\n",
      "P@1\t0.777\n",
      "R@1\t0.777\n"
     ]
    }
   ],
   "source": [
    "print_results(*model.test('testdata_with_labels_fastext_no_ngram.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fastext = model.predict(testdata_with_labels_fastext_no_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = [re.search(r'.+__(\\d+)',x[0]).group(1) for x in pred_fastext[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7635099586621571"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastest_no_ngram_accu = accuracy_score (test_labels,result)\n",
    "fastest_no_ngram_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t26134\n",
      "P@1\t0.776\n",
      "R@1\t0.776\n",
      "fastest_ngram_accu= 0.762532882375047\n"
     ]
    }
   ],
   "source": [
    "## model for fastext using data with ngram\n",
    "\n",
    "model_ngram = train_supervised(input = 'traindata_with_labels_fastext_ngram.txt',lr=0.8,dim=50,epoch=25,minCount=1,wordNgrams=3,loss=\"softmax\",verbose=2,thread = 3)\n",
    "print_results(*model_ngram.test('testdata_with_labels_fastext_ngram.txt'))\n",
    "pred_fastext_ngram = model_ngram.predict(testdata_with_labels_fastext_ngram)\n",
    "result_ngram = [re.search(r'.+__(\\d+)',x[0]).group(1) for x in pred_fastext_ngram[0]]\n",
    "fastest_ngram_accu = accuracy_score (test_labels,result_ngram)\n",
    "print('fastest_ngram_accu=',fastest_ngram_accu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    "## Overall it seems that MLP has the highest accuracy among all the algorithms tested above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
