{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "There was a kaggle style competition at my university to classify news articles using R language. The news articles had 23 labels that had been changed to be C1 to C23 and our goal was to achieve the highest F1 macro score. <br>\n",
    "Here for simplicity purpose we are just using accuracy as a metric for performance comparision. <br>\n",
    "So far we have preprocessed the given news articles and corresponding labels and made them ready to perform modelling on them. \n",
    "Preprocessing can be seen in this  <a href=\"https://github.com/VarunM24/data-science-portfolio/blob/master/Python/Machine%20Learning/text-classification/News-classification/pre-processing/News-Preprocessing.ipynb\" >notebook </a>.\n",
    "<br><br>In this notebook we will be performing modelling on the news articles using the following algorithms for comparison. \n",
    "#### - Naive bayes\n",
    "#### - Logistic regression\n",
    "#### - Linear SVM\n",
    "#### - Multilayer perceptron\n",
    "#### - CNN - - with/without ngram collocation and using pretrained wordembedding\n",
    "#### - CNN + LSTM -  with with/without collocation and using pretrained wordembedding\n",
    "#### - FastText \n",
    "<br>\n",
    "In various research papers these algorithms have been seen to have performed quite well which is why we chose them for comparison.\n",
    "The parameters to these algorithms were found out though either grid search or by testing out different configurations. The final configurations have been used here.\n",
    "<br> Originally this task was completed in R language. The same models have been reproduced here for the sake of improving proficiency in python and learning how to use different deep learning libraries in python for text classification.\n",
    "\n",
    "======================================================================================================================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier, LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation,GlobalAveragePooling1D\n",
    "from keras.callbacks import  EarlyStopping\n",
    "import re\n",
    "from fastText import train_supervised\n",
    "import urllib.request\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data_from_url(filepath):\n",
    "    ''' Reads news article document data from url\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    filepath : str\n",
    "        url to get the news article documents from\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    temp_data : list\n",
    "        list of documents that were read from the url given\n",
    "    \n",
    "    '''\n",
    "    fp = urllib.request.urlopen(filepath)\n",
    "    temp_data = fp.readlines()\n",
    "    temp_data = [x.decode(\"latin-1\").replace('\\n','').replace('\\r','') for x in  temp_data]\n",
    "    fp.close()\n",
    "    return temp_data\n",
    "\n",
    "def read_data1(filepath):\n",
    "    ''' Obtains news article documents from local file path\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    filepath : str\n",
    "        local file path to get the news article documents from\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    temp_data : list\n",
    "        list of documents that were read from the filepath given    \n",
    "    '''\n",
    "    temp_data = open(filepath,'r')\n",
    "    temp_data = temp_data.readlines()\n",
    "    temp_data = [x.replace('\\n','') for x in  temp_data]\n",
    "    return temp_data\n",
    "\n",
    "def read_label(filepath):\n",
    "    ''' Obtains news article documents labels from local file path or url\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    filepath : str\n",
    "        local file path or url to get the news article documents labels from\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    temp_data : list\n",
    "        list of labels of news documents that were read from the filepath/url given    \n",
    "    '''\n",
    "    temp_pd  = pd.read_csv(filepath)\n",
    "    temp_pd = temp_pd.y.str.replace('C','')\n",
    "    return list(temp_pd.values)\n",
    "\n",
    "def read_testing_label(filepath):\n",
    "    ''' Obtains news article documents test set labels from url\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    filepath : str\n",
    "        url to get the news article documents labels of test set \n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    temp_data : list\n",
    "        list of labels of test set news documents that were read from the url given    \n",
    "    '''\n",
    "    fp = urllib.request.urlopen(filepath)\n",
    "    temp_data = fp.readlines()\n",
    "    x = pd.DataFrame(temp_data)\n",
    "    str_df = x.stack().str.decode('utf-8').unstack()\n",
    "    str_df = str_df.loc[:,0].str.extract('.+C(\\d+)\\r\\n')\n",
    "    fp.close()\n",
    "    return  str_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VM\\AppData\\Roaming\\Python\\Python35\\site-packages\\ipykernel\\__main__.py:73: FutureWarning: currently extract(expand=None) means expand=False (return Index/Series/DataFrame) but in a future version of pandas this will be changed to expand=True (return DataFrame)\n"
     ]
    }
   ],
   "source": [
    "# Files are stored on Dropbox\n",
    "# Original unprocessed data - https://www.dropbox.com/sh/6hbhx9t5gbadox1/AACbFbyPSf2Gl_20Z6-vo0SIa?dl=1\n",
    "# Processed data - https://www.dropbox.com/sh/b21j3vrivynm3qq/AACQO-5IhVySCsEUfW5iGm7ca?dl=1\n",
    "\n",
    "train_data_ngram = read_data_from_url('https://www.dropbox.com/s/345f53i28jzgbwf/train_data1.txt?dl=1')\n",
    "train_data_no_ngram = read_data_from_url('https://www.dropbox.com/s/dg60mnmph5peo3j/train_data_no_ngram.txt?dl=1')\n",
    "val_data_ngram = read_data_from_url('https://www.dropbox.com/s/jhwoc3itepiru5n/validation_data1.txt?dl=1')\n",
    "val_data_no_ngram = read_data_from_url('https://www.dropbox.com/s/2gmv1u90gs1bcfe/validation_data_no_ngram.txt?dl=1')\n",
    "complete_data_ngram = read_data_from_url('https://www.dropbox.com/s/nwtja7kqt8zxeo0/complete_data1.txt?dl=1')\n",
    "complete_data_no_ngram = read_data_from_url ('https://www.dropbox.com/s/n8weciidr2p8w89/complete_data_no_ngram.txt?dl=1')\n",
    "train_label_ngram = read_label('https://www.dropbox.com/s/s6awj7z6u5twwzr/train_labels1.csv?dl=1')\n",
    "train_label_no_ngram = read_label('https://www.dropbox.com/s/zn6q7kjns2o8lde/train_labels_no_ngram.csv?dl=1')\n",
    "val_label_ngram = read_label('https://www.dropbox.com/s/yazczj4wylnwowt/validation_labels1.csv?dl=1')\n",
    "val_label_no_ngram  = read_label('https://www.dropbox.com/s/13s4sm2u4a8ze7n/validation_labels_no_ngram.csv?dl=1')\n",
    "test_labels = read_testing_label('https://www.dropbox.com/s/tufczh9achf84zr/testing_labels.txt?dl=1')\n",
    "\n",
    "test_data_ngram = read_data_from_url('https://www.dropbox.com/s/xusb0moohur7y8c/test_data1.txt?dl=1')\n",
    "test_data_no_ngram = read_data_from_url('https://www.dropbox.com/s/mywzv8o8v57nem9/test_data_no_ngram.txt?dl=1')\n",
    "    \n",
    "complete_labels_ngram =  train_label_ngram + val_label_ngram\n",
    "complete_labels_no_ngram = train_label_no_ngram + val_label_no_ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary to store different model performances\n",
    "df_performances = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using TF IDF\n",
    "#### First we will create models using TF IDF form of the data and evaluate their performance on validation data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating tf-idf matrix for ngram data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning the tfidf vectorizer to learn vocabulary and the id values of each word from training data\n",
    "tfidf_vectorizer = TfidfVectorizer(input = 'content', analyzer = 'word')\n",
    "tfidf_train_data_ngram = tfidf_vectorizer.fit_transform(complete_data_ngram)\n",
    "# transforming test data into idf vector\n",
    "tfidf_test_data_ngram = tfidf_vectorizer.transform(test_data_ngram) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating tf-idf matrix for data without ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# learning the tfidf vectorizer to learn vocabulary and the id values of each word from training data\n",
    "tfidf_vectorizer = TfidfVectorizer(input = 'content', analyzer = 'word')\n",
    "tfidf_train_data_no_ngram = tfidf_vectorizer.fit_transform(complete_data_no_ngram)\n",
    "# transforming test data into idf vector\n",
    "tfidf_test_data_no_ngram = tfidf_vectorizer.transform(test_data_no_ngram) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes classifier\n",
    "=================================\n",
    "### 5 fold Crossvalidation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a stratified shuffled split\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=321)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_MNB = MultinomialNB(alpha = 0.1)\n",
    "crossval_arr = cross_val_score(clf_MNB, tfidf_train_data_ngram, complete_labels_ngram, cv=sss)\n",
    "avg_nb = (sum(crossval_arr)/len(crossval_arr))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB 5 fold crossvalidation avg accuracy = 75.28 % \n"
     ]
    }
   ],
   "source": [
    "print ('NB 5 fold crossvalidation avg accuracy = {0:.2f} % '.format(avg_nb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_MNB = clf_MNB.fit(tfidf_train_data_ngram, complete_labels_ngram)\n",
    "predicted_nb = clf_MNB.predict(tfidf_test_data_ngram)\n",
    "nb_accu = accuracy_score (test_labels,predicted_nb) *100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NB test accuracy = 73.99 % \n"
     ]
    }
   ],
   "source": [
    "print ('NB test accuracy = {0:.2f} % '.format(nb_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to performance dictionary\n",
    "df_performances['NB_ngram'] = nb_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM classifier\n",
    "=======================\n",
    "### 5 fold Crossvalidation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_svm= OneVsOneClassifier(SGDClassifier(loss='hinge', penalty='l2',alpha=1e-4, max_iter=50, random_state=42))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM 5 fold crossvalidation avg accuracy =  77.95 % \n"
     ]
    }
   ],
   "source": [
    "crossval_arr_svm = cross_val_score(clf_svm, tfidf_train_data_ngram, complete_labels_ngram, cv=sss)\n",
    "avg_svm = (sum(crossval_arr_svm)/len(crossval_arr_svm))*100\n",
    "print ('SVM 5 fold crossvalidation avg accuracy =  {0:.2f} % '.format(avg_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM test accuracy = 7.7e+01 % \n"
     ]
    }
   ],
   "source": [
    "clf_svm = clf_svm.fit(tfidf_train_data_ngram, complete_labels_ngram)\n",
    "predicted_svm = clf_svm.predict(tfidf_test_data_ngram)\n",
    "svm_accu = accuracy_score (test_labels,predicted_svm)*100\n",
    "print ('SVM test accuracy = {0:.2f} % '.format(svm_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM test accuracy = 76.85 % \n"
     ]
    }
   ],
   "source": [
    "print ('SVM test accuracy = {0:.2f} % '.format(svm_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to performance dictionary\n",
    "df_performances['SVM_ngram'] = svm_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106443, 45367)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_train_data_ngram.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP Classifier\n",
    "========================\n",
    "### 2 Crossvalidation performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\VM\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "C:\\Users\\VM\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:564: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (2) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 neuron MLP 2 fold crossvalidation avg accuracy =   7.8e+01 % \n"
     ]
    }
   ],
   "source": [
    "# Creating a stratified shuffled split\n",
    "sss_mlp = StratifiedShuffleSplit(n_splits=2, test_size=0.2, random_state=321)\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,), learning_rate_init=0.001, max_iter=2)\n",
    "crossval_arr_mlp = cross_val_score(mlp_clf, tfidf_train_data_ngram, complete_labels_ngram, cv=sss_mlp)\n",
    "avg_mlp = (sum(crossval_arr_mlp)/len(crossval_arr_mlp))*100\n",
    "print ('100 neuron MLP 2 fold crossvalidation avg accuracy =   {0:.2f} % '.format(avg_mlp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 neuron MLP 2 fold crossvalidation avg accuracy =   78.16 % \n"
     ]
    }
   ],
   "source": [
    "print ('100 neuron MLP 2 fold crossvalidation avg accuracy =   {0:.2f} % '.format(avg_mlp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance on test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 neuron MLP test accuracy =   7.7e+01 % \n"
     ]
    }
   ],
   "source": [
    "mlp = mlp_clf.fit(tfidf_train_data_ngram, complete_labels_ngram)\n",
    "predicted_mlp = mlp.predict(tfidf_test_data_ngram)\n",
    "mlp_accu = accuracy_score (test_labels,predicted_mlp)*100\n",
    "print ('100 neuron MLP test accuracy =   {0:.2f} % '.format(mlp_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 neuron MLP test accuracy =   77.15 % \n"
     ]
    }
   ],
   "source": [
    "print ('100 neuron MLP test accuracy =   {0:.2f} % '.format(mlp_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to performance dictionary\n",
    "df_performances['MLP_100N_ngram'] = mlp_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression\n",
    "=========================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 5 fold crossvalidation avg accuracy =   7.8e+01 % \n"
     ]
    }
   ],
   "source": [
    "lr_clf = LogisticRegression()\n",
    "crossval_arr = cross_val_score(lr_clf, tfidf_train_data_ngram, complete_labels_ngram, cv=sss)\n",
    "avg_lr = (sum(crossval_arr)/len(crossval_arr))*100\n",
    "print ('LR 5 fold crossvalidation avg accuracy =   {0:.2f} % '.format(avg_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR 5 fold crossvalidation avg accuracy =   78.03 % \n"
     ]
    }
   ],
   "source": [
    "print ('LR 5 fold crossvalidation avg accuracy =   {0:.2f} % '.format(avg_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR test accuracy =   7.7e+01 % \n"
     ]
    }
   ],
   "source": [
    "lr_clf = lr_clf.fit(tfidf_train_data_ngram, complete_labels_ngram)\n",
    "predicted_lr = lr_clf.predict(tfidf_test_data_ngram)\n",
    "lr_accu = accuracy_score (test_labels,predicted_lr) *100\n",
    "print ('LR test accuracy =   {0:.2} % '.format(lr_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('LR test accuracy =   {0:.2f} % '.format(lr_accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding to performance dictionary\n",
    "df_performances['LR_ngram'] = lr_accu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using pretrained GloVe word embedding\n",
    " GloVe is a type of model which after getting trained can be used to create word embedding i.e. words representation in a vector space which can be used as an alternate representation for words and used as input in deep learning models.  We will use pretrained GloVe model which has been trained on news as well as wikipedia which is a corpus similar to our own. Since these vector representation can retain some sort of semantic information, using it may yield better performance.<br><br>\n",
    " For deep learning techniques, we intend to use transfer learning by using pretrained word embedding in our model and to let the model fine tune the weights and learn from our corpus as well to save time and improve performance. And finally these word embeddings will be used as input features to various models. <br><br>\n",
    " The pretrained wordembedding file name is glove.6B.100d and can be downloaded from :\n",
    "https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading Glove word embedding in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGlove(Glove_filepath):\n",
    "    ''' Loads glove pretrained word embeddings from local path\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    Glove_filepath : str\n",
    "        local path to glove pretrained word embedding\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    embeddings_index: dict\n",
    "        contains words and their corresponding vector values\n",
    "    '''\n",
    "    embeddings_index = {}\n",
    "    f = open(Glove_filepath, encoding=\"utf8\")\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0] ## The first entry is the word\n",
    "        coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print('GloVe data loaded')\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadGlove_from_url(Glove_filepath):\n",
    "    ''' Loads glove pretrained word embeddings from url\n",
    "    \n",
    "    Parameters:\n",
    "    ----------\n",
    "    Glove_filepath : str\n",
    "        local path to glove pretrained word embedding\n",
    "        \n",
    "    Returns:\n",
    "    -------\n",
    "    embeddings_index: dict\n",
    "        contains words and their corresponding vector values\n",
    "    '''\n",
    "    \n",
    "    fp = urllib.request.urlopen(Glove_filepath)\n",
    "#     temp_data = fp.readlines()\n",
    "#     temp_data = [x.decode(\"latin-1\").replace('\\n','').replace('\\r','') for x in  temp_data]\n",
    "    \n",
    "    embeddings_index = {}\n",
    "    for line in fp.readlines():\n",
    "        values = line.decode('utf-8').split(' ')\n",
    "        word = values[0] ## The first entry is the word\n",
    "        coefs = np.asarray(values[1:], dtype='float32') ## These are the vecotrs representing the embedding for the word\n",
    "        embeddings_index[word] = coefs\n",
    "    fp.close()\n",
    "    print('GloVe data loaded')\n",
    "    return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GloVe data loaded\n"
     ]
    }
   ],
   "source": [
    "# glove_dict = loadGlove('glove.6B.100d.txt')\n",
    "# File is 300mb ... download from url may take some time\n",
    "\n",
    "# glove_dict = loadGlove_from_url('https://www.dropbox.com/s/f25667uh1yzjsxa/glove.6B.50d.txt?dl=1')\n",
    "glove_dict = loadGlove_from_url('https://www.dropbox.com/s/dxhnb2o27ijegks/glove.6B.100d.txt?dl=1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN\n",
    "### We will first be using data without ngrams for CNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the data for CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 116063 unique words.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "# Getting word embedding \n",
    "MAX_SEQUENCE_LENGTH = 130\n",
    "\n",
    "# Using tokenizer to learn from complete training dataset (validation  + training )\n",
    "tokenizer1 = Tokenizer()\n",
    "tokenizer1.fit_on_texts(complete_data_no_ngram)\n",
    "\n",
    "# Converting documents to sequence of tokens\n",
    "complete_train_no_ngram = tokenizer1.texts_to_sequences(complete_data_no_ngram)\n",
    "complete_test_no_ngram = tokenizer1.texts_to_sequences(test_data_no_ngram)\n",
    "\n",
    "word_index_no_ngram = tokenizer1.word_index\n",
    "vocab_size = len(word_index_no_ngram)\n",
    "max_words = vocab_size\n",
    "\n",
    "\n",
    "print('Found %s unique words.' % len(word_index_no_ngram))\n",
    "\n",
    "#padding the sequences incase sequences are shorter than max sequence length\n",
    "complete_train_no_ngram_padded = pad_sequences(complete_train_no_ngram, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "complete_test_no_ngram_padded = pad_sequences(complete_test_no_ngram, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# converting labels to int and then reducing their value by 1 as the first class will be 0 not 1 (required by keras)\n",
    "complete_labels_no_ngram = np.array(list(map(int, complete_labels_no_ngram)))\n",
    "complete_labels_no_ngram = complete_labels_no_ngram - 1\n",
    "\n",
    "test_labels = np.array(list(map(int, test_labels)))\n",
    "test_labels = test_labels - 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing the embedding matrix for embedding layer  in CNN using pretrained glove word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding\n",
    "from keras.initializers import Constant\n",
    "\n",
    "\n",
    "# Preparing embedding matrix \n",
    "EMBEDDING_DIM = glove_dict.get('a').shape[0]\n",
    "num_words = max_words + 1\n",
    "embedding_matrix_no_ngram = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index_no_ngram.items():\n",
    "    if i>max_words:\n",
    "        continue\n",
    "    embedding_vector = glove_dict.get(word)\n",
    "    if(embedding_vector is not None):\n",
    "        embedding_matrix_no_ngram[i] = embedding_vector\n",
    "\n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix_no_ngram),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True) # We set trainable to True because it gives better performance by optimizing the weights more\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What does an embedding matrix look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116064, 100)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "print(embedding_matrix_no_ngram.shape)\n",
    "plt.plot(embedding_matrix_no_ngram[16])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating  the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(embedding_layer)\n",
    "model.add(GlobalAveragePooling1D())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Conv1D(64, 5, activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=4))\n",
    "#model.add(LSTM(100))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(23, activation='softmax'))\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106443"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(complete_train_no_ngram_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training the CNN model on validation + training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 101120 samples, validate on 5323 samples\n",
      "Epoch 1/7\n",
      "101120/101120 [==============================] - 79s 782us/step - loss: 1.2012 - acc: 0.6744 - val_loss: 0.8751 - val_acc: 0.7509\n",
      "Epoch 2/7\n",
      "101120/101120 [==============================] - 73s 724us/step - loss: 0.7631 - acc: 0.7726 - val_loss: 0.7832 - val_acc: 0.7665\n",
      "Epoch 3/7\n",
      "101120/101120 [==============================] - 74s 734us/step - loss: 0.6279 - acc: 0.8068 - val_loss: 0.7438 - val_acc: 0.7759\n",
      "Epoch 4/7\n",
      "101120/101120 [==============================] - 81s 799us/step - loss: 0.5247 - acc: 0.8366 - val_loss: 0.7416 - val_acc: 0.7794\n",
      "Epoch 5/7\n",
      "101120/101120 [==============================] - 72s 713us/step - loss: 0.4340 - acc: 0.8631 - val_loss: 0.7642 - val_acc: 0.7785\n",
      "Epoch 6/7\n",
      "101120/101120 [==============================] - 72s 710us/step - loss: 0.3528 - acc: 0.8889 - val_loss: 0.7950 - val_acc: 0.7785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25499edb438>"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "earlystopping_callback = EarlyStopping(monitor='val_loss', patience=2,restore_best_weights=True)\n",
    "lr_reduce_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)        \n",
    "model.fit(complete_train_no_ngram_padded,complete_labels_no_ngram\n",
    "          ,batch_size = 32, validation_split=0.05, epochs = 7, callbacks = [earlystopping_callback,lr_reduce_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN trained on data without ngram accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7622322435174747"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_ngram_cnn_pred7 = model.predict(complete_test_no_ngram_padded)\n",
    "no_ngram_cnn_pred7 = list(map(np.argmax, no_ngram_cnn_pred7))\n",
    "no_ngram_cnn_accu7 = accuracy_score (test_labels,no_ngram_cnn_pred7)\n",
    "no_ngram_cnn_accu7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN trained on data with validaiton data without ngram accuracy - test accuracy =   76.22 % \n"
     ]
    }
   ],
   "source": [
    "print ('CNN trained on data with validaiton data without ngram accuracy - test accuracy =   {0:.2f} % '.format(no_ngram_cnn_accu7 *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now training on complete data to find accuracy on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "106443/106443 [==============================] - 76s 712us/step - loss: 0.8538 - acc: 0.7782\n",
      "Epoch 2/3\n",
      "106443/106443 [==============================] - 76s 711us/step - loss: 0.5192 - acc: 0.8458\n",
      "Epoch 3/3\n",
      "106443/106443 [==============================] - 77s 728us/step - loss: 0.4188 - acc: 0.8725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x254ab49ac18>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model2 = Sequential()\n",
    "model2.add(embedding_layer)\n",
    "model2.add(GlobalAveragePooling1D())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Conv1D(64, 5, activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=4))\n",
    "#model.add(LSTM(100))\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "model2.add(Dense(23, activation='softmax'))\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "#earlystopping_callback = EarlyStopping(monitor='val_loss', patience=2,restore_best_weights=True)\n",
    "#lr_reduce_callback = ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=2, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)        \n",
    "model2.fit(complete_train_no_ngram_padded,complete_labels_no_ngram\n",
    "          ,batch_size = 32, epochs = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7499060503570086"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_ngram_cnn_pred = model2.predict(complete_test_no_ngram_padded)\n",
    "no_ngram_cnn_pred1 = list(map(np.argmax, no_ngram_cnn_pred))\n",
    "no_ngram_cnn_accu = accuracy_score (test_labels,no_ngram_cnn_pred1)\n",
    "no_ngram_cnn_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN trained on complete data without validaiton data without ngram accuracy - test accuracy =   74.99 % \n"
     ]
    }
   ],
   "source": [
    "print ('CNN trained on complete data without validaiton data without ngram accuracy - test accuracy =   {0:.2f} % '.format(no_ngram_cnn_accu *100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now training CNN using data with ngram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparing data and embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 40747 unique words.\n"
     ]
    }
   ],
   "source": [
    "#complete_data_ngram\n",
    "#complete_labels_ngram\n",
    "\n",
    "\n",
    "# Getting word embedding \n",
    "MAX_SEQUENCE_LENGTH = 130\n",
    "\n",
    "tokenizer2 = Tokenizer()\n",
    "tokenizer2.fit_on_texts(complete_data_ngram)\n",
    "\n",
    "complete_train_ngram = tokenizer2.texts_to_sequences(complete_data_ngram)\n",
    "complete_test_ngram = tokenizer2.texts_to_sequences(test_data_ngram)\n",
    "\n",
    "word_index_ngram = tokenizer2.word_index\n",
    "vocab_size = len(word_index_ngram)\n",
    "max_words = vocab_size\n",
    "\n",
    "\n",
    "print('Found %s unique words.' % len(word_index_ngram))\n",
    "\n",
    "#padding the sequences\n",
    "complete_train_ngram_padded = pad_sequences(complete_train_ngram, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "complete_test_ngram_padded = pad_sequences(complete_test_ngram, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "complete_labels_ngram = np.array(list(map(int, complete_labels_ngram)))\n",
    "complete_labels_ngram = complete_labels_ngram - 1\n",
    "\n",
    "#test_labels = np.array(list(map(int, test_labels)))\n",
    "#test_labels = test_labels - 1\n",
    "\n",
    "# Preparing embedding matrix \n",
    "EMBEDDING_DIM = glove_dict.get('a').shape[0]\n",
    "num_words = max_words + 1\n",
    "embedding_matrix_ngram = np.zeros((num_words, EMBEDDING_DIM))\n",
    "\n",
    "for word, i in word_index_ngram.items():\n",
    "    if i>max_words:\n",
    "        continue\n",
    "    embedding_vector = glove_dict.get(word)\n",
    "    if(embedding_vector is not None):\n",
    "        embedding_matrix_ngram[i] = embedding_vector\n",
    "        \n",
    "embedding_layer = Embedding(num_words,\n",
    "                            EMBEDDING_DIM,\n",
    "                            embeddings_initializer=Constant(embedding_matrix_ngram),\n",
    "                            input_length=MAX_SEQUENCE_LENGTH,\n",
    "                            trainable=True) # We set trainable to True because it gives better performance by optimizing the weights more\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating the CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ngram = Sequential()\n",
    "model_ngram.add(embedding_layer)\n",
    "model_ngram.add(GlobalAveragePooling1D())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Conv1D(64, 5, activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=4))\n",
    "#model.add(LSTM(100))\n",
    "model_ngram.add(Dense(64, activation='relu'))\n",
    "model_ngram.add(Dense(23, activation='softmax'))\n",
    "model_ngram.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training CNN model using ngram data with validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 101120 samples, validate on 5323 samples\n",
      "Epoch 1/7\n",
      "101120/101120 [==============================] - 35s 351us/step - loss: 1.1927 - acc: 0.6773 - val_loss: 0.9041 - val_acc: 0.7299\n",
      "Epoch 2/7\n",
      "101120/101120 [==============================] - 35s 344us/step - loss: 0.7665 - acc: 0.7730 - val_loss: 0.8141 - val_acc: 0.7509\n",
      "Epoch 3/7\n",
      "101120/101120 [==============================] - 37s 367us/step - loss: 0.6421 - acc: 0.8033 - val_loss: 0.7875 - val_acc: 0.7518\n",
      "Epoch 4/7\n",
      "101120/101120 [==============================] - 34s 339us/step - loss: 0.5479 - acc: 0.8295 - val_loss: 0.7811 - val_acc: 0.7684\n",
      "Epoch 5/7\n",
      "101120/101120 [==============================] - 34s 338us/step - loss: 0.4675 - acc: 0.8523 - val_loss: 0.7939 - val_acc: 0.7644\n",
      "Epoch 6/7\n",
      "101120/101120 [==============================] - 34s 338us/step - loss: 0.3939 - acc: 0.8756 - val_loss: 0.8467 - val_acc: 0.7577\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x254c909a0b8>"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "earlystopping_callback = EarlyStopping(monitor='val_loss', patience=2,restore_best_weights=True)\n",
    "\n",
    "model_ngram.fit(complete_train_ngram_padded,complete_labels_ngram\n",
    "          ,batch_size = 32, validation_split=0.05, epochs = 7, callbacks = [earlystopping_callback,lr_reduce_callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN with ngram data test accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7630965802329951"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_cnn_pred7 = model_ngram.predict(complete_test_ngram_padded)\n",
    "ngram_cnn_pred7 = list(map(np.argmax, ngram_cnn_pred7))\n",
    "ngram_cnn_accu7 = accuracy_score (test_labels,ngram_cnn_pred7)\n",
    "ngram_cnn_accu7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN trained with validaiton data with ngram accuracy - test accuracy =   76.31 % \n"
     ]
    }
   ],
   "source": [
    "print ('CNN trained with validaiton data with ngram accuracy - test accuracy =   {0:.2f} % '.format(ngram_cnn_accu7 *100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN with ngram data - training On complete training data without validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "106443/106443 [==============================] - 38s 358us/step - loss: 0.8840 - acc: 0.7724\n",
      "Epoch 2/3\n",
      "106443/106443 [==============================] - 36s 334us/step - loss: 0.5537 - acc: 0.8347\n",
      "Epoch 3/3\n",
      "106443/106443 [==============================] - 35s 327us/step - loss: 0.4630 - acc: 0.8579\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x254d52702b0>"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ngram2 = Sequential()\n",
    "model_ngram2.add(embedding_layer)\n",
    "model_ngram2.add(GlobalAveragePooling1D())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Conv1D(64, 5, activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=4))\n",
    "#model.add(LSTM(100))\n",
    "model_ngram2.add(Dense(64, activation='relu'))\n",
    "model_ngram2.add(Dense(23, activation='softmax'))\n",
    "model_ngram2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_ngram2.fit(complete_train_ngram_padded,complete_labels_ngram\n",
    "          ,batch_size = 32, epochs = 3 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy - CNN trained on complete training data with ngram without validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7552423900789177"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_cnn_pred = model_ngram2.predict(complete_test_ngram_padded)\n",
    "ngram_cnn_pred1 = list(map(np.argmax, ngram_cnn_pred))\n",
    "ngram_cnn_accu = accuracy_score (test_labels,ngram_cnn_pred1)\n",
    "ngram_cnn_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN trained on complete training data with ngram without validation data with 3 epochs- test accuracy =   75.52 % \n"
     ]
    }
   ],
   "source": [
    "print ('CNN trained on complete training data with ngram without validation data with 3 epochs- test accuracy =   {0:.2f} % '.format(ngram_cnn_accu *100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training with complete ngram training data with 5 epochs instead of 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "106443/106443 [==============================] - 36s 336us/step - loss: 0.7704 - acc: 0.8018\n",
      "Epoch 2/5\n",
      "106443/106443 [==============================] - 35s 330us/step - loss: 0.4509 - acc: 0.8662\n",
      "Epoch 3/5\n",
      "106443/106443 [==============================] - 35s 328us/step - loss: 0.3733 - acc: 0.8872\n",
      "Epoch 4/5\n",
      "106443/106443 [==============================] - 35s 330us/step - loss: 0.3148 - acc: 0.9041\n",
      "Epoch 5/5\n",
      "106443/106443 [==============================] - 35s 328us/step - loss: 0.2662 - acc: 0.9190\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x254e45c2e80>"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ngram5 = Sequential()\n",
    "model_ngram5.add(embedding_layer)\n",
    "model_ngram5.add(GlobalAveragePooling1D())\n",
    "#model.add(Dropout(0.2))\n",
    "#model.add(Conv1D(64, 5, activation='relu'))\n",
    "#model.add(MaxPooling1D(pool_size=4))\n",
    "#model.add(LSTM(100))\n",
    "model_ngram5.add(Dense(64, activation='relu'))\n",
    "model_ngram5.add(Dense(23, activation='softmax'))\n",
    "model_ngram5.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_ngram5.fit(complete_train_ngram_padded,complete_labels_ngram\n",
    "          ,batch_size = 32, epochs = 5 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7416009019165727"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_cnn_pred5 = model_ngram5.predict(complete_test_ngram_padded)\n",
    "ngram_cnn_pred5 = list(map(np.argmax, ngram_cnn_pred5))\n",
    "ngram_cnn_accu5 = accuracy_score (test_labels,ngram_cnn_pred5)\n",
    "ngram_cnn_accu5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN trained on complete training data with ngram without validation data with 5 epochs- test accuracy =   74.16 % \n"
     ]
    }
   ],
   "source": [
    "print ('CNN trained on complete training data with ngram without validation data with 5 epochs- test accuracy =   {0:.2f} % '.format(ngram_cnn_accu5 *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performances['CNN'] = max([ngram_cnn_accu5*100,ngram_cnn_accu*100, ngram_cnn_accu7*100 , no_ngram_cnn_accu*100, no_ngram_cnn_accu7*100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see from above experiments.... the test accuracy varies a lot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets try with CNN + LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "106443/106443 [==============================] - 221s 2ms/step - loss: 0.5575 - acc: 0.8280\n",
      "Epoch 2/3\n",
      "106443/106443 [==============================] - 210s 2ms/step - loss: 0.3794 - acc: 0.8813\n",
      "Epoch 3/3\n",
      "106443/106443 [==============================] - 210s 2ms/step - loss: 0.3175 - acc: 0.9002\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x254ea21dfd0>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "\n",
    "model_ngram_cnnlstm = Sequential()\n",
    "model_ngram_cnnlstm.add(embedding_layer)\n",
    "model_ngram_cnnlstm.add(Dropout(0.2))\n",
    "model_ngram_cnnlstm.add(Conv1D(64, 5, activation='relu'))\n",
    "model_ngram_cnnlstm.add(MaxPooling1D(pool_size=4))\n",
    "model_ngram_cnnlstm.add(LSTM(180))\n",
    "model_ngram_cnnlstm.add(Dense(64, activation='relu'))\n",
    "model_ngram_cnnlstm.add(Dense(23, activation='softmax'))\n",
    "model_ngram_cnnlstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_ngram_cnnlstm.fit(complete_train_ngram_padded,complete_labels_ngram\n",
    "          ,batch_size = 32, epochs = 3 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106443, 130)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_train_ngram_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7164599774520857"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_cnnlstm_pred = model_ngram_cnnlstm.predict(complete_test_ngram_padded)\n",
    "ngram_cnnlstm_pred = list(map(np.argmax, ngram_cnnlstm_pred))\n",
    "ngram_cnnlstm_accu = accuracy_score (test_labels,ngram_cnnlstm_pred)\n",
    "ngram_cnnlstm_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN+LSTM trained on complete training data with ngram without validation data with 3 epochs- test accuracy =   71.65 % \n"
     ]
    }
   ],
   "source": [
    "print ('CNN+LSTM trained on complete training data with ngram without validation data with 3 epochs- test accuracy =   {0:.2f} % '.format(ngram_cnnlstm_accu *100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN+ LSTM with different configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 103249 samples, validate on 3194 samples\n",
      "Epoch 1/5\n",
      "103249/103249 [==============================] - 204s 2ms/step - loss: 0.4741 - acc: 0.8560 - val_loss: 0.4280 - val_acc: 0.8745\n",
      "Epoch 2/5\n",
      "103249/103249 [==============================] - 201s 2ms/step - loss: 0.2933 - acc: 0.9083 - val_loss: 0.4324 - val_acc: 0.8691\n",
      "Epoch 3/5\n",
      "103249/103249 [==============================] - 201s 2ms/step - loss: 0.2274 - acc: 0.9304 - val_loss: 0.4572 - val_acc: 0.8654\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x254ff279ef0>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "\n",
    "model_ngram_cnnlstm1 = Sequential()\n",
    "model_ngram_cnnlstm1.add(embedding_layer)\n",
    "#model_ngram_cnnlstm.add(Dropout(0.2))\n",
    "model_ngram_cnnlstm1.add(Conv1D(64, 5, activation='relu'))\n",
    "model_ngram_cnnlstm1.add(MaxPooling1D(pool_size=4))\n",
    "model_ngram_cnnlstm1.add(LSTM(150))\n",
    "model_ngram_cnnlstm1.add(Dense(23, activation='softmax'))\n",
    "model_ngram_cnnlstm1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_ngram_cnnlstm1.fit(complete_train_ngram_padded,complete_labels_ngram\n",
    "          ,batch_size = 32, validation_split=0.03, epochs = 5, callbacks = [earlystopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7227358136039083"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngram_cnnlstm_pred1 = model_ngram_cnnlstm1.predict(complete_test_ngram_padded)\n",
    "ngram_cnnlstm_pred1 = list(map(np.argmax, ngram_cnnlstm_pred1))\n",
    "ngram_cnnlstm_accu1 = accuracy_score (test_labels,ngram_cnnlstm_pred1)\n",
    "ngram_cnnlstm_accu1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN+LSTM trained on  training data with ngram with validation data with 5 epochs- test accuracy =   72.27 % \n"
     ]
    }
   ],
   "source": [
    "print ('CNN+LSTM trained on  training data with ngram with validation data with 5 epochs- test accuracy =   {0:.2f} % '.format(ngram_cnnlstm_accu1 *100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN+ LSTM without ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 103249 samples, validate on 3194 samples\n",
      "Epoch 1/5\n",
      "103249/103249 [==============================] - 203s 2ms/step - loss: 1.2894 - acc: 0.6076 - val_loss: 0.9104 - val_acc: 0.7239\n",
      "Epoch 2/5\n",
      "103249/103249 [==============================] - 201s 2ms/step - loss: 0.7810 - acc: 0.7600 - val_loss: 0.8478 - val_acc: 0.7370\n",
      "Epoch 3/5\n",
      "103249/103249 [==============================] - 202s 2ms/step - loss: 0.6383 - acc: 0.8020 - val_loss: 0.8180 - val_acc: 0.7614\n",
      "Epoch 4/5\n",
      "103249/103249 [==============================] - 201s 2ms/step - loss: 0.5222 - acc: 0.8366 - val_loss: 0.8693 - val_acc: 0.7467\n",
      "Epoch 5/5\n",
      "103249/103249 [==============================] - 202s 2ms/step - loss: 0.4138 - acc: 0.8703 - val_loss: 0.9574 - val_acc: 0.7448\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x256019f0be0>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "\n",
    "model_no_ngram_cnnlstm = Sequential()\n",
    "model_no_ngram_cnnlstm.add(embedding_layer)\n",
    "#model_ngram_cnnlstm.add(GlobalAveragePooling1D())\n",
    "#model_ngram_cnnlstm.add(Dropout(0.2))\n",
    "model_no_ngram_cnnlstm.add(Conv1D(64, 5, activation='relu'))\n",
    "model_no_ngram_cnnlstm.add(MaxPooling1D(pool_size=4))\n",
    "model_no_ngram_cnnlstm.add(LSTM(150))\n",
    "model_no_ngram_cnnlstm.add(Dense(23, activation='softmax'))\n",
    "model_no_ngram_cnnlstm.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_no_ngram_cnnlstm.fit(complete_train_no_ngram_padded,complete_labels_no_ngram\n",
    "          ,batch_size = 32, validation_split=0.03, epochs = 5, callbacks = [earlystopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7372416384817738"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_ngram_cnnlstm_pred = model_no_ngram_cnnlstm.predict(complete_test_no_ngram_padded)\n",
    "no_ngram_cnnlstm_pred = list(map(np.argmax, no_ngram_cnnlstm_pred))\n",
    "no_ngram_cnnlstm_accu = accuracy_score (test_labels,no_ngram_cnnlstm_pred)\n",
    "no_ngram_cnnlstm_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN+LSTM trained on  training data without ngram with validation data with 5 epochs- test accuracy =   73.72 % \n"
     ]
    }
   ],
   "source": [
    "print ('CNN+LSTM trained on  training data without ngram with validation data with 5 epochs- test accuracy =   {0:.2f} % '.format(no_ngram_cnnlstm_accu *100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### with dropout and another dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 103249 samples, validate on 3194 samples\n",
      "Epoch 1/3\n",
      "103249/103249 [==============================] - 207s 2ms/step - loss: 0.9405 - acc: 0.7176 - val_loss: 0.8810 - val_acc: 0.7408\n",
      "Epoch 2/3\n",
      "103249/103249 [==============================] - 205s 2ms/step - loss: 0.6765 - acc: 0.7940 - val_loss: 0.8499 - val_acc: 0.7574\n",
      "Epoch 3/3\n",
      "103249/103249 [==============================] - 206s 2ms/step - loss: 0.6020 - acc: 0.8136 - val_loss: 0.8315 - val_acc: 0.7574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x256080a95c0>"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "\n",
    "model_no_ngram_cnnlstm2 = Sequential()\n",
    "model_no_ngram_cnnlstm2.add(embedding_layer)\n",
    "#model_ngram_cnnlstm.add(GlobalAveragePooling1D())\n",
    "model_no_ngram_cnnlstm2.add(Dropout(0.2))\n",
    "model_no_ngram_cnnlstm2.add(Conv1D(64, 5, activation='relu'))\n",
    "model_no_ngram_cnnlstm2.add(MaxPooling1D(pool_size=4))\n",
    "model_no_ngram_cnnlstm2.add(LSTM(150))\n",
    "model_no_ngram_cnnlstm2.add(Dense(64, activation='relu'))\n",
    "model_no_ngram_cnnlstm2.add(Dense(23, activation='softmax'))\n",
    "model_no_ngram_cnnlstm2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_no_ngram_cnnlstm2.fit(complete_train_no_ngram_padded,complete_labels_no_ngram\n",
    "          ,batch_size = 32, validation_split=0.03, epochs = 3, callbacks = [earlystopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7452085682074409"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_ngram_cnnlstm_pred2 = model_no_ngram_cnnlstm2.predict(complete_test_no_ngram_padded)\n",
    "no_ngram_cnnlstm_pred2 = list(map(np.argmax, no_ngram_cnnlstm_pred2))\n",
    "no_ngram_cnnlstm_accu2 = accuracy_score (test_labels,no_ngram_cnnlstm_pred2)\n",
    "no_ngram_cnnlstm_accu2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN+LSTM trained on  training data without ngram with validation data with 3 epochs, dropout and another dense layer- test accuracy =   74.52 % \n"
     ]
    }
   ],
   "source": [
    "print ('CNN+LSTM trained on  training data without ngram with validation data with 3 epochs, dropout and another dense layer- test accuracy =   {0:.2f} % '.format(no_ngram_cnnlstm_accu2 *100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Increasing the validation split  to 10%  just to observe if results vary if i change the. Even though the training set used to train the data will decrease but will it have any counter intuitive result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 95798 samples, validate on 10645 samples\n",
      "Epoch 1/3\n",
      "95798/95798 [==============================] - 197s 2ms/step - loss: 0.8231 - acc: 0.7559 - val_loss: 0.7088 - val_acc: 0.7875\n",
      "Epoch 2/3\n",
      "95798/95798 [==============================] - 195s 2ms/step - loss: 0.5836 - acc: 0.8229 - val_loss: 0.6893 - val_acc: 0.7967\n",
      "Epoch 3/3\n",
      "95798/95798 [==============================] - 196s 2ms/step - loss: 0.5161 - acc: 0.8418 - val_loss: 0.7135 - val_acc: 0.7946\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2560fbddf98>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel_size = 5\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "\n",
    "model_no_ngram_cnnlstm3 = Sequential()\n",
    "model_no_ngram_cnnlstm3.add(embedding_layer)\n",
    "#model_ngram_cnnlstm.add(GlobalAveragePooling1D())\n",
    "model_no_ngram_cnnlstm3.add(Dropout(0.2))\n",
    "model_no_ngram_cnnlstm3.add(Conv1D(64, 5, activation='relu'))\n",
    "model_no_ngram_cnnlstm3.add(MaxPooling1D(pool_size=4))\n",
    "model_no_ngram_cnnlstm3.add(LSTM(150))\n",
    "model_no_ngram_cnnlstm3.add(Dense(64, activation='relu'))\n",
    "model_no_ngram_cnnlstm3.add(Dense(23, activation='softmax'))\n",
    "model_no_ngram_cnnlstm3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "model_no_ngram_cnnlstm3.fit(complete_train_no_ngram_padded,complete_labels_no_ngram\n",
    "          ,batch_size = 32, validation_split=0.1, epochs = 3, callbacks = [earlystopping_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.740924464487035"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_ngram_cnnlstm_pred3 = model_no_ngram_cnnlstm3.predict(complete_test_no_ngram_padded)\n",
    "no_ngram_cnnlstm_pred3 = list(map(np.argmax, no_ngram_cnnlstm_pred3))\n",
    "no_ngram_cnnlstm_accu3 = accuracy_score (test_labels,no_ngram_cnnlstm_pred3)\n",
    "no_ngram_cnnlstm_accu3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN+LSTM trained on  training data without ngram with 10% validation data with 3 epochs, dropout and another dense layer- test accuracy =   74.09 % \n"
     ]
    }
   ],
   "source": [
    "print ('CNN+LSTM trained on  training data without ngram with 10% validation data with 3 epochs, dropout and another dense layer- test accuracy =   {0:.2f} % '.format(no_ngram_cnnlstm_accu3 *100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performances['CNN_LSTM'] = max([no_ngram_cnnlstm_accu3*100,no_ngram_cnnlstm_accu2*100, no_ngram_cnnlstm_accu*100 , ngram_cnnlstm_accu1*100, ngram_cnnlstm_accu*100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN+ LSTM Summary\n",
    "From above tests we can notice that the best accuracy was in the first attempt with ngrams using the complete dataset as training data\n",
    "\n",
    "=========================================================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fast text model \n",
    "================<br>\n",
    "A model developed by Facebook which can be used for text classification\n",
    "\n",
    "It can be installed from https://github.com/facebookresearch/fastText/tree/master/python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will transform the input into required format. Each document is required to be preceded by \\_\\_label\\_\\_[label] and a space.<br>\n",
    " Example : \\_\\_label_\\_19 year_man ballarat hospital minor_injury crash tree maryborough crash midnight intersection timor road police_investigate cause collision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ngrams can be specified in fasttext api we will use data without ngrams initially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText without ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking the complete data and labels and concatenating them as required by API\n",
    "# Adding __label__ to each string in labels list\n",
    "training_labels_fastext_no_ngram = ['__label__'+str(a)+' ' for a in complete_labels_no_ngram]\n",
    "traindata_with_labels_fastext_no_ngram = [\"{}{}\".format(labels_,docs_) for  labels_,docs_ in zip(training_labels_fastext_no_ngram,complete_data_no_ngram)]\n",
    "# Doing same for test data without ngram\n",
    "testing_labels_fastext = ['__label__'+str(a)+' ' for a in test_labels]\n",
    "testdata_with_labels_fastext_no_ngram = [\"{}{}\".format(labels_,docs_) for  labels_,docs_ in zip(testing_labels_fastext,test_data_no_ngram)]\n",
    "\n",
    "\n",
    "# we can also do the same with data with ngram just for testing purposes for later modelling\n",
    "training_labels_fastext_ngram = ['__label__'+str(a)+' ' for a in complete_labels_ngram]\n",
    "traindata_with_labels_fastext_ngram = [\"{}{}\".format(labels_,docs_) for  labels_,docs_ in zip(training_labels_fastext_ngram,complete_data_ngram)]\n",
    "\n",
    "# Doing same for test data with ngram\n",
    "testdata_with_labels_fastext_ngram = [\"{}{}\".format(labels_,docs_) for  labels_,docs_ in zip(testing_labels_fastext,test_data_ngram)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing to disk since the input for fast text model accepts a filepath\n",
    "with open('traindata_with_labels_fastext_no_ngram.txt','w+') as o_fh:\n",
    "    for doc in traindata_with_labels_fastext_no_ngram:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()                \n",
    " \n",
    "    \n",
    "with open('testdata_with_labels_fastext_no_ngram.txt','w+') as o_fh:\n",
    "    for doc in testdata_with_labels_fastext_no_ngram:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()                \n",
    "\n",
    "with open('traindata_with_labels_fastext_ngram.txt','w+') as o_fh:\n",
    "    for doc in traindata_with_labels_fastext_ngram:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()                \n",
    "\n",
    "with open('testdata_with_labels_fastext_ngram.txt','w+') as o_fh:\n",
    "    for doc in testdata_with_labels_fastext_ngram:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training FastText Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# From previous testing we already have optimal parameters for this corpus\n",
    "# to make a model so we will directly be checking accuracy on test data\n",
    "def print_results(N, p, r):\n",
    "    print(\"N\\t\" + str(N))\n",
    "    print(\"P@{}\\t{:.3f}\".format(1, p))\n",
    "    print(\"R@{}\\t{:.3f}\".format(1, r))\n",
    "model = train_supervised(input = 'traindata_with_labels_fastext_no_ngram.txt',lr=0.8,dim=50,epoch=25,minCount=1,wordNgrams=3,loss=\"softmax\",verbose=2,thread = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t26134\n",
      "P@1\t0.777\n",
      "R@1\t0.777\n"
     ]
    }
   ],
   "source": [
    "print_results(*model.test('testdata_with_labels_fastext_no_ngram.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test accuracy for FastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fastext = model.predict(testdata_with_labels_fastext_no_ngram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "result = [int(re.search(r'.+__(\\d+)',x[0]).group(1)) for x in pred_fastext[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7631341600901916"
      ]
     },
     "execution_count": 208,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fastest_no_ngram_accu = accuracy_score (test_labels,result)\n",
    "fastest_no_ngram_accu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText without using ngram data (but specifying ngram in api)- test accuracy =   76.31 % \n"
     ]
    }
   ],
   "source": [
    "print ('FastText without using ngram data (but specifying ngram in api)- test accuracy =   {0:.2f} % '.format(fastest_no_ngram_accu *100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FastText with ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N\t26134\n",
      "P@1\t0.775\n",
      "R@1\t0.775\n",
      "fastest_ngram_accu= 0.7612927470875611\n"
     ]
    }
   ],
   "source": [
    "## model for fastext using data with ngram\n",
    "\n",
    "model_ngram = train_supervised(input = 'traindata_with_labels_fastext_ngram.txt',lr=0.8,dim=50,epoch=25,minCount=1,wordNgrams=3,loss=\"softmax\",verbose=2,thread = 3)\n",
    "print_results(*model_ngram.test('testdata_with_labels_fastext_ngram.txt'))\n",
    "pred_fastext_ngram = model_ngram.predict(testdata_with_labels_fastext_ngram)\n",
    "result_ngram = [int(re.search(r'.+__(\\d+)',x[0]).group(1)) for x in pred_fastext_ngram[0]]\n",
    "fastest_ngram_accu = accuracy_score (test_labels,result_ngram)\n",
    "print('fastest_ngram_accu=',fastest_ngram_accu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText using ngram data (also specifying ngram in api)- test accuracy =   76.13 % \n"
     ]
    }
   ],
   "source": [
    "print ('FastText using ngram data (also specifying ngram in api)- test accuracy =   {0:.2f} % '.format(fastest_ngram_accu *100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results of all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performances['FastText'] = max([fastest_no_ngram_accu*100,fastest_ngram_accu*100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_performances1= pd.DataFrame.from_dict(data=df_performances,orient=\"index\").reset_index()\n",
    "df_performances1.columns=['Model','Accuracy']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_performances1.sort_values(by='Accuracy').plot.bar(x='Model',y=\"Accuracy\",title=\"Results\",legend=False)\n",
    "plt.ylim(70, 80)\n",
    "plt.ylabel(\"Accuracy %\")\n",
    "plt.xticks(rotation=40)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary:\n",
    " Overall it seems that MLP has the highest accuracy among all the algorithms tested above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
