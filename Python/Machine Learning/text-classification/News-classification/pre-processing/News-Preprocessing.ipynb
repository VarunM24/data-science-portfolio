{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environment - Jupyter Notebook, Python 3\n",
    "\n",
    "## Introduction\n",
    "### In this notebook we will perform preprocessing news articles for further modelling task.\n",
    "### Since executing this notebook can take a long time there are Checkpoints along the way from where pickle file can be loaded to resume execution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "from TextPreprocess import Preprocess\n",
    "import nltk\n",
    "import pickle\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "from nltk.tokenize import MWETokenizer\n",
    "from nltk import word_tokenize,sent_tokenize\n",
    "from itertools import chain\n",
    "from nltk.probability import FreqDist\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file1 = open('../Data/original/training_docs.txt','r',encoding='UTF-8')\n",
    "file2 = open('../Data/original/training_labels_final.txt','r',encoding='UTF-8')\n",
    "file3 = open('../Data/original/testing_docs.txt','r',encoding='UTF-8')\n",
    "raw_train_data = file1.readlines()\n",
    "raw_train_label = file2.readlines()\n",
    "raw_test_data = file3.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess = Preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the training file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID tr_doc_1\\n',\n",
       " 'TEXT Two German tourists have been found safe and well after spending almost six hours lost in rugged rainforest at Finch Hatton Gorge, west of Mackay, last night. It is the same area a young Mackay man fell or jumped to his death last week. Sergeant Jon Purcell says rescuers located the missing pair just before midnight AEST.\\n',\n",
       " 'EOD\\n',\n",
       " '\\n',\n",
       " 'ID tr_doc_2\\n',\n",
       " 'TEXT ACT police have seized a rare drug during a raid of a Florey home. Police found a number of syringes filled with the drug Ox-Blood, which is a form of amphetamine. They also found a number of bags believed to contain crystal methamphetamine. A 29-year-old woman has been charged with a number of offences and has faced court this morning. Acting Sergeant Matt Varley says it is only the third time the drug has been found in the territory. \"It\\'s actually a bi-product of the amphetamine manufacturing process whereby normal powders and crystals are produced,\" he said. \"It\\'s a liquid methamphetamine and it contains red phospherous and iodine giving it it\\'s red colour.\"\\n',\n",
       " 'EOD\\n',\n",
       " '\\n',\n",
       " 'ID tr_doc_3\\n',\n",
       " 'TEXT A 50-year-old Brisbane man has been charged with fraud after he allegedly posed as a taxi driver. Police say a number of people reported the man after getting into his car and noticing it did not have a meter, radio or computer. The man will appear in Brisbane Magistrates Court later in the month.\\n']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID te_doc_1\\n',\n",
       " \"TEXT The Police Royal Commission in Western Australia is hearing evidence from the first serving officer to testify about corrupt activities in the service. The officer, code-named L-8, has been in the service for more than 30 years. He reached the rank of Inspector. He has testified that his improper behaviour began in 1979 when he and another detective took $200 from an armed robbery suspect. He has told the commission that he subsequently assaulted suspects, and made up a statement that was put before a court in an armed robbery case. L-8 is the eighth officer to 'roll over' since the Royal Commission started mid last year.\\n\",\n",
       " 'EOD\\n',\n",
       " '\\n',\n",
       " 'ID te_doc_2\\n',\n",
       " \"TEXT The Northern Territory Government says it is the Queensland Government's responsibility to explain why it denied a transfer to Alice Springs prisoner Tommy Neale. Neale is mounting a Supreme Court challenge over the decision. Tommy Neale was convicted of murder in Mount Isa in 1981. He later requested a transfer to Alice Springs where, under Territory law, he is now serving a mandatory life sentence with no parole. Neale's lawyer says he would have been able to apply for parole in Queensland nine years ago. Kim Kilvington from Central Australian Aboriginal Legal Aid has accused the Queensland and Northern Territory governments of stonewalling efforts to find out why a third application for Neale to be transferred back to Queensland was rejected last year. The Territory's Attorney General Peter Toyne says he has received correspondence from Neale's representatives but he says it is up to the Queensland Government to explain its decision. Queensland's Corrective Services Minister Tony McGrady has previously said the application was refused because of Neale's poor behaviour in prison and it could not be sustained on family grounds. His office says that statement still stands. .\\n\",\n",
       " 'EOD\\n',\n",
       " '\\n',\n",
       " 'ID te_doc_3\\n',\n",
       " 'TEXT A group of hepatitis C sufferers, who were infected through blood transfusions, are a step closer to learning the identity of the donors. The group is taking action against the Australian Red Cross Society over its screening processes. The New South Wales Supreme Court today ruled the lawyers for the three sufferers should be given access to details regarding the identity of the people who donated the blood more than 15 years ago. The lawyers say they want to find out just how comprehensive the screening process was at the time. However the court has delayed their access to the donors for several weeks until the Red Cross has determined whether it will appeal against the decision.\\n']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_test_data[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the labels file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tr_doc_1 C1\\n',\n",
       " 'tr_doc_2 C1\\n',\n",
       " 'tr_doc_3 C1\\n',\n",
       " 'tr_doc_4 C1\\n',\n",
       " 'tr_doc_5 C1\\n',\n",
       " 'tr_doc_6 C1\\n',\n",
       " 'tr_doc_7 C1\\n',\n",
       " 'tr_doc_8 C1\\n',\n",
       " 'tr_doc_9 C1\\n',\n",
       " 'tr_doc_10 C1\\n']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_label[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning up labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels for tarining data in order\n",
    "labels = [each.strip('\\n').split()[1] for each in raw_train_label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1', 'C1']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[1:9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean up text data in training and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the '\\n' items\n",
    "raw_train_data = [i for i in raw_train_data if i != '\\n']\n",
    "raw_test_data = [i for i in raw_test_data if i != '\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ID tr_doc_1\\n',\n",
       " 'TEXT Two German tourists have been found safe and well after spending almost six hours lost in rugged rainforest at Finch Hatton Gorge, west of Mackay, last night. It is the same area a young Mackay man fell or jumped to his death last week. Sergeant Jon Purcell says rescuers located the missing pair just before midnight AEST.\\n',\n",
       " 'EOD\\n',\n",
       " 'ID tr_doc_2\\n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_train_data[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting train and test data into paragraph form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs_train = preprocess.get_in_paragraph(raw_train_data)\n",
    "paragraphs_test = preprocess.get_in_paragraph(raw_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Two German tourists have been found safe and well after spending almost six hours lost in rugged rainforest at Finch Hatton Gorge, west of Mackay, last night. It is the same area a young Mackay man fell or jumped to his death last week. Sergeant Jon Purcell says rescuers located the missing pair just before midnight AEST.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking...results\n",
    "paragraphs_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking max and min length of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data max length 93206\n",
      "training data min length 1\n",
      "number of news article with length greater than 1000 =  49913\n",
      "number of news article with length smaller than 10 =  1287\n",
      "number of news article with length smaller than 20 =  1438\n"
     ]
    }
   ],
   "source": [
    "max_len = max([len(i) for i in paragraphs_train])\n",
    "print('training data max length',max_len)\n",
    "min_len =min([len(i) for i in paragraphs_train])\n",
    "print('training data min length',min_len)\n",
    "# number of documents with words above 1000\n",
    "large_len =len([i for i in paragraphs_train if len(i)>1000])\n",
    "print('number of news article with length greater than 1000 = ',large_len)\n",
    "#number of documents with words below 10\n",
    "very_small_len = len([i for i in paragraphs_train if len(i)<10])\n",
    "print('number of news article with length smaller than 10 = ',very_small_len)\n",
    "small_len = len([i for i in paragraphs_train if len(i)<20])\n",
    "print('number of news article with length smaller than 20 = ',small_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting data to sentences for POS tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Two German tourists have been found safe and well after spending almost six hours lost in rugged rainforest at Finch Hatton Gorge, west of Mackay, last night.',\n",
       " 'It is the same area a young Mackay man fell or jumped to his death last week.',\n",
       " 'Sergeant Jon Purcell says rescuers located the missing pair just before midnight AEST.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading punkt sentence segmenter\n",
    "sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "#paragraphs_train\n",
    "#paragraphs_test\n",
    "sentences_train =[0]*len(paragraphs_train)\n",
    "sentences_test =[0]*len(paragraphs_test)\n",
    "for i in range(len(paragraphs_train)):\n",
    "        sentences_train[i] = sent_detector.tokenize(paragraphs_train[i].strip())\n",
    "        \n",
    "for i in range(len(paragraphs_test)):\n",
    "        sentences_test[i] = sent_detector.tokenize(paragraphs_test[i].strip())\n",
    "\n",
    "sentences_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding POS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.add_POS(sentences_train)\n",
    "preprocess.add_POS(sentences_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Two', 'CD'),\n",
       "  ('German', 'JJ'),\n",
       "  ('tourists', 'NNS'),\n",
       "  ('have', 'VBP'),\n",
       "  ('been', 'VBN'),\n",
       "  ('found', 'VBN'),\n",
       "  ('safe', 'JJ'),\n",
       "  ('and', 'CC'),\n",
       "  ('well', 'RB'),\n",
       "  ('after', 'IN'),\n",
       "  ('spending', 'VBG'),\n",
       "  ('almost', 'RB'),\n",
       "  ('six', 'CD'),\n",
       "  ('hours', 'NNS'),\n",
       "  ('lost', 'VBN'),\n",
       "  ('in', 'IN'),\n",
       "  ('rugged', 'JJ'),\n",
       "  ('rainforest', 'NN'),\n",
       "  ('at', 'IN'),\n",
       "  ('Finch', 'NNP'),\n",
       "  ('Hatton', 'NNP'),\n",
       "  ('Gorge', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('west', 'NN'),\n",
       "  ('of', 'IN'),\n",
       "  ('Mackay', 'NNP'),\n",
       "  (',', ','),\n",
       "  ('last', 'JJ'),\n",
       "  ('night', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('It', 'PRP'),\n",
       "  ('is', 'VBZ'),\n",
       "  ('the', 'DT'),\n",
       "  ('same', 'JJ'),\n",
       "  ('area', 'NN'),\n",
       "  ('a', 'DT'),\n",
       "  ('young', 'JJ'),\n",
       "  ('Mackay', 'NNP'),\n",
       "  ('man', 'NN'),\n",
       "  ('fell', 'VBD'),\n",
       "  ('or', 'CC'),\n",
       "  ('jumped', 'VBD'),\n",
       "  ('to', 'TO'),\n",
       "  ('his', 'PRP$'),\n",
       "  ('death', 'NN'),\n",
       "  ('last', 'JJ'),\n",
       "  ('week', 'NN'),\n",
       "  ('.', '.')],\n",
       " [('Sergeant', 'JJ'),\n",
       "  ('Jon', 'NNP'),\n",
       "  ('Purcell', 'NNP'),\n",
       "  ('says', 'VBZ'),\n",
       "  ('rescuers', 'NNS'),\n",
       "  ('located', 'VBD'),\n",
       "  ('the', 'DT'),\n",
       "  ('missing', 'VBG'),\n",
       "  ('pair', 'NN'),\n",
       "  ('just', 'RB'),\n",
       "  ('before', 'RB'),\n",
       "  ('midnight', 'JJ'),\n",
       "  ('AEST', 'NNP'),\n",
       "  ('.', '.')]]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatize training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmazied_docs_train = preprocess.lemmatization(sentences_train)\n",
    "lemmazied_docs_test = preprocess.lemmatization(sentences_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since POS and lemmatization takes so long we can save the result in pickle file and continue later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( lemmazied_docs_train, open( \"lemmazied_docs_train_lemmatized_unfiltered_byPOS.p\", \"wb\" ) )\n",
    "pickle.dump( lemmazied_docs_test, open( \"lemmazied_docs_test_lemmatized_unfiltered_byPOS.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing lemmatized training and test documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmazied_docs_train = pickle.load( open( \"lemmazied_docs_train_lemmatized_unfiltered_byPOS.p\", \"rb\" ) )\n",
    "lemmazied_docs_test = pickle.load( open( \"lemmazied_docs_test_lemmatized_unfiltered_byPOS.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting 1 word articles to UKNWN token to reduce features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of token, 1\n",
      "index - 4553\n",
      "length of token, 1\n",
      "index - 4583\n",
      "length of token, 1\n",
      "index - 10221\n",
      "length of token, 1\n",
      "index - 10237\n",
      "length of token, 1\n",
      "index - 10471\n",
      "length of token, 1\n",
      "index - 11138\n",
      "length of token, 1\n",
      "index - 11591\n",
      "length of token, 1\n",
      "index - 13465\n",
      "length of token, 1\n",
      "index - 18280\n",
      "length of token, 1\n",
      "index - 18851\n",
      "total number  15\n",
      "no of one word docs 15\n",
      "percentage of one word docs 0.0005636978579481398\n",
      "some of the indexes of 1 word docs \n",
      " [4553, 4583, 10221, 10237, 10471, 11138, 11591, 13465, 18280, 18851]\n",
      "length of token, 1\n",
      "index - 4235\n",
      "length of token, 1\n",
      "index - 4238\n",
      "length of token, 1\n",
      "index - 4265\n",
      "length of token, 1\n",
      "index - 8749\n",
      "length of token, 1\n",
      "index - 8756\n",
      "length of token, 1\n",
      "index - 8818\n",
      "length of token, 1\n",
      "index - 18393\n",
      "length of token, 1\n",
      "index - 18427\n",
      "length of token, 1\n",
      "index - 19020\n",
      "length of token, 1\n",
      "index - 25067\n",
      "total number  56\n",
      "no of one word docs 56\n",
      "percentage of one word docs 0.0005260932876133214\n",
      "some of the indexes of 1 word docs \n",
      " [4235, 4238, 4265, 8749, 8756, 8818, 18393, 18427, 19020, 25067]\n"
     ]
    }
   ],
   "source": [
    "new_test_docs = preprocess.change_oneworddoc_to(lemmazied_docs_test,\"UKNWN\")\n",
    "new_train_docs = preprocess.change_oneworddoc_to(lemmazied_docs_train,\"UKNWN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_new_train_docs = preprocess.convert_lowercase(new_train_docs)\n",
    "lc_new_test_docs = preprocess.convert_lowercase(new_test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding trigram and bi-gram collocations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting to get all tokens\n",
      "Got all tokens\n",
      "getting all bigram collocations\n",
      "got all bigram collocations\n",
      "getting all trigram collocations\n",
      "got all trigram collocations\n",
      "remving stopwords from all bigram collocations\n",
      "remved stopwords from all bigram collocations\n",
      "remving stopwords from all trigram collocations\n",
      "remved stopwords from all trigram collocations\n",
      "remving symbols from all collocations\n",
      "remved symbols from all collocations\n",
      "[('sakineh', 'mohammadi', 'ashtiani'), ('sutopo', 'purwo', 'nugroho'), ('ku', 'klux', 'klan'), ('inverness', 'caledonian', 'thistle'), ('fathur', 'rohman', 'al-ghozi'), ('se', 'og', 'hoer'), ('taur', 'matan', 'ruak'), ('abd', 'al-rahim', 'al-nashiri'), ('khagendra', 'thapa', 'magar'), ('tuilaepa', 'sailele', 'malielegaoi'), ('gro', 'harlem', 'brundtland'), ('kwa', 'zulu', 'natal'), ('kyodo', 'senpaku', 'kaisha'), ('elmer', 'funke', 'kupper'), ('bovine', 'spongiform', 'encephalopathy'), ('masai', 'moses', 'ndiema'), ('bran', 'nue', 'dae'), ('kiri', 'te', 'kanawa'), ('humam', 'khalil', 'abu-mulal')]\n",
      "[('technologically', 'advanced'), ('terrey', 'hills'), ('francis', 'joyon'), ('mack', 'mccormack'), ('vladimir', 'putin'), ('smoky', 'haze'), ('witch', 'hunters'), ('ludwig', 'scotty'), ('chubu', 'electric'), ('knight', 'riders'), ('eric', 'akoto'), ('tranmere', 'rovers'), ('merri', 'rose'), ('alia', 'trucking'), ('off-spinner', 'omari'), ('atherton', 'tablelands'), ('joey', 'barton'), ('motor', 'neurone'), ('bret', 'michaels'), ('gillian', 'calvert')]\n",
      "All done - total collocations = \n",
      "11013\n"
     ]
    }
   ],
   "source": [
    "# using ngramlist2\n",
    "ngramlist2 = preprocess.get_bi_tri_collocations_filtered_freq_bmi_perc(tokens=lc_new_train_docs,\n",
    "                                                            trigram=True, N_best_bigram=7000,\n",
    "                                                            N_best_trigram=5000,\n",
    "                                                           min_corpus_bigram_freq=3,\n",
    "                                                   min_corpus_trigram_freq=3,\n",
    "                                           remove_stopwords=True,remove_symbols=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Removing stopwords to reduce features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.remove_stopwords(lc_new_train_docs)\n",
    "preprocess.remove_stopwords(lc_new_test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data\n",
    "preprocess.remove_punctuation(lc_new_train_docs)\n",
    "# train data\n",
    "preprocess.remove_punctuation(lc_new_test_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the ngrams into the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocess.introduce_n_grams_in_docs(lc_new_train_docs,ngramlist2)\n",
    "preprocess.introduce_n_grams_in_docs(lc_new_test_docs,ngramlist2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing words which occur less than 5 times "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_doc_threshold 5\n",
      "<FreqDist with 149955 samples and 9074448 outcomes>\n"
     ]
    }
   ],
   "source": [
    "req_filt_words = preprocess.filtered_words_by_perc_occur_corpus_more_less_than(list_tokens=lc_new_train_docs,perc_value=5,percent=False,more = False)\n",
    "latest_ngramlist = preprocess.remove_words_from_ngramlist(ngramlist2,req_filt_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lenth of doclist =  106445\n",
      "total words in filter list 105970\n",
      "--- 2.0369150638580322 seconds for removal ---\n",
      "--- 6.797841548919678 seconds for MWE ---\n",
      "finishing removing word from train\n"
     ]
    }
   ],
   "source": [
    "#Removing filter words from Training docs\n",
    "lc_new_train_docs = preprocess.remove_words_tuples_corpus(lc_new_train_docs,latest_ngramlist,req_filt_words)\n",
    "print('finishing removing word from train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lenth of doclist =  26610\n",
      "total words in filter list 105970\n",
      "--- 0.5163042545318604 seconds for removal ---\n",
      "--- 1.4766912460327148 seconds for MWE ---\n",
      "finishing removing word from testlist\n"
     ]
    }
   ],
   "source": [
    "#Removing filter words from test docs\n",
    "lc_new_test_docs = preprocess.remove_words_tuples_corpus(lc_new_test_docs,latest_ngramlist,req_filt_words)\n",
    "print('finishing removing word from testlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first find those high frequency words among all the text\n",
    "all_tokens = []\n",
    "for each in lc_new_train_docs:\n",
    "    all_tokens += list(set(each))\n",
    "all_frequency = dict(nltk.FreqDist(all_tokens))\n",
    "might_remove = list(dict(nltk.FreqDist(all_tokens).most_common(2000)).keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing words which have low skewness in its distribution over all classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_dict = {}\n",
    "\n",
    "for i in range(len(lc_new_train_docs)):\n",
    "    word_set = list(set(lc_new_train_docs[i]))\n",
    "    for each in word_set:\n",
    "        if labels[i] in a_dict:\n",
    "            a_dict[labels[i]].append(each)\n",
    "        else:\n",
    "            a_dict[labels[i]] = [each]\n",
    "    \n",
    "frequent_words = []\n",
    "for k,v in a_dict.items():\n",
    "    v = np.random.permutation(v)\n",
    "    frequent_word = set(v[:2000])\n",
    "    frequent_words += list(frequent_word)\n",
    "    \n",
    "a = dict(nltk.FreqDist(frequent_words).most_common(1000))\n",
    "\n",
    "# if a word is both frequent among the whole text and frequent among all classes (>=20), then remove\n",
    "to_remove = set()\n",
    "for each in might_remove:\n",
    "    if each in a:\n",
    "        if a[each] >=22:\n",
    "            to_remove.add(each)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the distribution of classes even\n",
    "\n",
    "label_count = dict(nltk.FreqDist(labels))\n",
    "\n",
    "ratio = [0]*23\n",
    "for k,v in label_count.items():\n",
    "    k = int(k.replace(' ','')[1:]) -1\n",
    "    ratio[k] = 5520/v\n",
    "ratio = np.array(ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness_matrix = {}\n",
    "for k,v in a_dict.items():\n",
    "    k = int(k.replace(' ','')[1:]) -1\n",
    "    for each in v:\n",
    "        if each in skewness_matrix:\n",
    "            skewness_matrix[each][k] +=1\n",
    "        else:\n",
    "            skewness_matrix[each] = [0]*23\n",
    "            skewness_matrix[each][k] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "skewness_dict = {}\n",
    "for k,v in skewness_matrix.items():\n",
    "    v = np.array(v)\n",
    "    average = np.std(v)/np.mean(v)\n",
    "    skewness_dict[k] = average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>skewness</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a-b-c</td>\n",
       "      <td>4.690416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a-c-t</td>\n",
       "      <td>4.690416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a-c-t-u</td>\n",
       "      <td>4.690416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a-f-l</td>\n",
       "      <td>4.690416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a-f-l_footballer_liam</td>\n",
       "      <td>4.690416</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    word  skewness\n",
       "0                  a-b-c  4.690416\n",
       "1                  a-c-t  4.690416\n",
       "2                a-c-t-u  4.690416\n",
       "3                  a-f-l  4.690416\n",
       "4  a-f-l_footballer_liam  4.690416"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_dict_skewness = pd.read_table('skewness_dict.txt',delim_whitespace =True,header=None)\n",
    "df_dict_skewness = pd.Series(skewness_dict).to_frame('ColumnName').reset_index()\n",
    "df_dict_skewness.rename(columns={0: 'word', 1: 'skewness'}, inplace=True)\n",
    "df_dict_skewness.columns = { 'word',  'skewness'}\n",
    "df_dict_skewness.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     abandon\n",
       "1       abide\n",
       "2     ability\n",
       "3        able\n",
       "4    absolute\n",
       "Name: word, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_rmv = df_dict_skewness[df_dict_skewness.skewness<0.6].word.reset_index().word\n",
    "to_rmv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_rmv_set = set(list(to_rmv)+list(to_remove))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lenth of doclist =  106445\n",
      "total words in filter list 874\n",
      "--- 1.44642972946167 seconds for removal ---\n",
      "--- 5.749646902084351 seconds for MWE ---\n"
     ]
    }
   ],
   "source": [
    "# Removing the low skewness words\n",
    "final_train_doclist = preprocess.remove_words_tuples_corpus(lc_new_train_docs,ngramlist2,to_rmv_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lenth of doclist =  26610\n",
      "total words in filter list 874\n",
      "--- 0.4021146297454834 seconds for removal ---\n",
      "--- 2.850888252258301 seconds for MWE ---\n"
     ]
    }
   ],
   "source": [
    "final_test_doclist = preprocess.remove_words_tuples_corpus(lc_new_test_docs,ngramlist2,to_rmv_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for empty documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of token, 0\n",
      "index - 15311\n",
      "length of token, 0\n",
      "index - 70829\n",
      "total number  2\n"
     ]
    }
   ],
   "source": [
    "final_train_doclist1 = list(final_train_doclist) \n",
    "less_than_10_idx = preprocess.too_short_item(final_train_doclist, 1,-1,1000000)\n",
    "final_train_doclist = [final_train_doclist[i] for i in range(len(final_train_doclist1)) if i not in less_than_10_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels1 = list(labels)\n",
    "train_labels_new = [labels[i] for i in range(len(labels1)) if i not in less_than_10_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total number  0\n"
     ]
    }
   ],
   "source": [
    "less_than_10_idx_test = preprocess.too_short_item(final_test_doclist, 1,-1,1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict_final,revocab_dict_final = preprocess.create_vocab_revocab_dict(final_train_doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48799"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking length\n",
    "len(vocab_dict_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert token list of documents back into whole document list of sentences for modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_doclist_concated =  preprocess.finalizing_document_list(final_train_doclist)\n",
    "test_doclist_concated =  preprocess.finalizing_document_list(final_test_doclist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['german tourist safe hour lose rugged rainforest finch hatton gorge west mackay night area young mackay man fell jump death sergeant jon purcell rescuer locate miss pair midnight aest',\n",
       " 'police seize rare drug raid florey home police syrinx drug form amphetamine bag contain crystal methamphetamine woman charge offence face court acting sergeant matt varley third drug territory amphetamine manufacturing whereby powder crystal produce liquid methamphetamine contain iodine colour',\n",
       " 'brisbane man charge fraud allegedly pose taxi driver police man car meter computer man brisbane magistrates court']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_doclist_concated[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "106443"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_doclist_concated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checkpoint 2 \n",
    "### Saving final result of pre-processing for modelling task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( training_doclist_concated, open( \"training_doclist_concated.p\", \"wb\" ) )\n",
    "pickle.dump( test_doclist_concated, open( \"test_doclist_concated.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump( train_labels_new, open( \"train_labels_new.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing final result of preprocessing for modelling tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lemmazied_docs_train = pickle.load( open( \"lemmazied_docs_train_lemmatized_unfiltered_byPOS.p\", \"rb\" ) )\n",
    "# lemmazied_docs_test = pickle.load( open( \"lemmazied_docs_test_lemmatized_unfiltered_byPOS.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the training data into validation and training data and also writing to txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx1 = np.random.permutation(len(training_doclist_concated))\n",
    "x_train1 = [training_doclist_concated[i] for i in idx1]\n",
    "y_train1 = [train_labels_new[i] for i in idx1]\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=321)\n",
    "for train_index, test_index in sss.split(x_train1, y_train1):\n",
    "    X_train, X_test = [x_train1[ind] for ind in train_index ], [x_train1[ind] for ind in test_index ]\n",
    "    y_train, y_test = [y_train1[ind] for ind in train_index ], [y_train1[ind] for ind in test_index ]\n",
    "    break\n",
    "\n",
    "complete_doc = X_train+X_test\n",
    "\n",
    "# writing to file for keras in r to read\n",
    "        \n",
    "    \n",
    "with open('../Data/processed/complete_data1.txt','w+') as o_fh:\n",
    "    for doc in complete_doc:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()                \n",
    "   \n",
    "with open('../Data/processed/train_data1.txt','w+') as o_fh:\n",
    "    for doc in X_train:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()  \n",
    "\n",
    "\n",
    "with open('../Data/processed/validation_data1.txt','w+') as o_fh:\n",
    "    for doc in X_test:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()  \n",
    "\n",
    "label_df_train=pd.DataFrame(y_train,columns=['y'])\n",
    "label_df_validation=pd.DataFrame(y_test,columns=['y'])\n",
    "label_df_train.to_csv('../Data/processed/train_labels1.csv')\n",
    "label_df_validation.to_csv('../Data/processed/validation_labels1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving processed test data as well in form of a txt file\n",
    "with open('../Data/processed/test_data1.txt','w+') as o_fh:\n",
    "    for doc in test_doclist_concated:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Also creating a set of processed data without ngram collocations for evaluation purpose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing data from pickle saved at checkpoint 1\n",
    "lemmazied_docs_train_eval = pickle.load( open( \"lemmazied_docs_train_lemmatized_unfiltered_byPOS.p\", \"rb\" ) )\n",
    "lemmazied_docs_test_eval = pickle.load( open( \"lemmazied_docs_test_lemmatized_unfiltered_byPOS.p\", \"rb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of token, 1\n",
      "index - 4553\n",
      "length of token, 1\n",
      "index - 4583\n",
      "length of token, 1\n",
      "index - 10221\n",
      "length of token, 1\n",
      "index - 10237\n",
      "length of token, 1\n",
      "index - 10471\n",
      "length of token, 1\n",
      "index - 11138\n",
      "length of token, 1\n",
      "index - 11591\n",
      "length of token, 1\n",
      "index - 13465\n",
      "length of token, 1\n",
      "index - 18280\n",
      "length of token, 1\n",
      "index - 18851\n",
      "total number  15\n",
      "no of one word docs 15\n",
      "percentage of one word docs 0.0005636978579481398\n",
      "some of the indexes of 1 word docs \n",
      " [4553, 4583, 10221, 10237, 10471, 11138, 11591, 13465, 18280, 18851]\n",
      "length of token, 1\n",
      "index - 4235\n",
      "length of token, 1\n",
      "index - 4238\n",
      "length of token, 1\n",
      "index - 4265\n",
      "length of token, 1\n",
      "index - 8749\n",
      "length of token, 1\n",
      "index - 8756\n",
      "length of token, 1\n",
      "index - 8818\n",
      "length of token, 1\n",
      "index - 18393\n",
      "length of token, 1\n",
      "index - 18427\n",
      "length of token, 1\n",
      "index - 19020\n",
      "length of token, 1\n",
      "index - 25067\n",
      "total number  56\n",
      "no of one word docs 56\n",
      "percentage of one word docs 0.0005260932876133214\n",
      "some of the indexes of 1 word docs \n",
      " [4235, 4238, 4265, 8749, 8756, 8818, 18393, 18427, 19020, 25067]\n"
     ]
    }
   ],
   "source": [
    "# Changing one word documents to UNKWN\n",
    "new_test_docs_eval = preprocess.change_oneworddoc_to(lemmazied_docs_test_eval,\"UKNWN\")\n",
    "new_train_docs_eval = preprocess.change_oneworddoc_to(lemmazied_docs_train_eval,\"UKNWN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_train_docs_eval = preprocess.convert_lowercase(new_train_docs_eval)\n",
    "new_test_docs_eval = preprocess.convert_lowercase(new_test_docs_eval)\n",
    "# removing stopwords and punctuations\n",
    "preprocess.remove_stopwords(new_test_docs_eval)\n",
    "preprocess.remove_stopwords(new_train_docs_eval)\n",
    "# test data\n",
    "preprocess.remove_punctuation(new_test_docs_eval)\n",
    "# train data\n",
    "preprocess.remove_punctuation(new_train_docs_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing words which occur less than 5 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_of_doc_threshold 5\n",
      "<FreqDist with 141844 samples and 9146696 outcomes>\n"
     ]
    }
   ],
   "source": [
    "ngramlist_eval=[]\n",
    "req_filt_words_eval = preprocess.filtered_words_by_perc_occur_corpus_more_less_than(list_tokens=new_train_docs_eval,perc_value=5,percent=False,more = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lenth of doclist =  106445\n",
      "total words in filter list 99053\n",
      "--- 11.630670547485352 seconds for removal ---\n",
      "--- 7.522034645080566 seconds for MWE ---\n",
      "finishing removing word from train\n"
     ]
    }
   ],
   "source": [
    "#Removing filter words from Training docs\n",
    "new_train_docs_eval = preprocess.remove_words_tuples_corpus(new_train_docs_eval,ngramlist_eval,req_filt_words_eval)\n",
    "print('finishing removing word from train')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lenth of doclist =  26610\n",
      "total words in filter list 99053\n",
      "--- 0.7798235416412354 seconds for removal ---\n",
      "--- 1.4874277114868164 seconds for MWE ---\n",
      "finishing removing word from testlist\n"
     ]
    }
   ],
   "source": [
    "#Removing filter words from test docs\n",
    "new_test_docs_eval = preprocess.remove_words_tuples_corpus(new_test_docs_eval,ngramlist_eval,req_filt_words_eval)\n",
    "print('finishing removing word from testlist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first find those high frequency words among all the text\n",
    "all_tokens = []\n",
    "for each in new_train_docs_eval:\n",
    "    all_tokens += list(set(each))\n",
    "all_frequency = dict(nltk.FreqDist(all_tokens))\n",
    "might_remove = list(dict(nltk.FreqDist(all_tokens).most_common(2000)).keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total lenth of doclist =  106445\n",
      "total words in filter list 875\n",
      "--- 8.6093008518219 seconds for removal ---\n",
      "--- 4.351987361907959 seconds for MWE ---\n",
      "total lenth of doclist =  26610\n",
      "total words in filter list 875\n",
      "--- 0.5055673122406006 seconds for removal ---\n",
      "--- 1.1956005096435547 seconds for MWE ---\n"
     ]
    }
   ],
   "source": [
    "# Removing low skewness (classwise) words \n",
    "a_dict = {}\n",
    "\n",
    "for i in range(len(new_train_docs_eval)):\n",
    "    word_set = list(set(new_train_docs_eval[i]))\n",
    "    for each in word_set:\n",
    "        if labels[i] in a_dict:\n",
    "            a_dict[labels[i]].append(each)\n",
    "        else:\n",
    "            a_dict[labels[i]] = [each]\n",
    "    \n",
    "frequent_words = []\n",
    "for k,v in a_dict.items():\n",
    "    v = np.random.permutation(v)\n",
    "    frequent_word = set(v[:2000])\n",
    "    frequent_words += list(frequent_word)\n",
    "    \n",
    "a = dict(nltk.FreqDist(frequent_words).most_common(1000))\n",
    "\n",
    "# if a word is both frequent among the whole text and frequent among all classes (>=20), then remove\n",
    "to_remove = set()\n",
    "for each in might_remove:\n",
    "    if each in a:\n",
    "        if a[each] >=22:\n",
    "            to_remove.add(each)\n",
    "# make the distribution of classes even\n",
    "\n",
    "label_count = dict(nltk.FreqDist(labels))\n",
    "\n",
    "ratio = [0]*23\n",
    "for k,v in label_count.items():\n",
    "    k = int(k.replace(' ','')[1:]) -1\n",
    "    ratio[k] = 5520/v\n",
    "ratio = np.array(ratio)\n",
    "\n",
    "skewness_matrix = {}\n",
    "for k,v in a_dict.items():\n",
    "    k = int(k.replace(' ','')[1:]) -1\n",
    "    for each in v:\n",
    "        if each in skewness_matrix:\n",
    "            skewness_matrix[each][k] +=1\n",
    "        else:\n",
    "            skewness_matrix[each] = [0]*23\n",
    "            skewness_matrix[each][k] +=1\n",
    "            \n",
    "skewness_dict = {}\n",
    "for k,v in skewness_matrix.items():\n",
    "    v = np.array(v)\n",
    "    average = np.std(v)/np.mean(v)\n",
    "    skewness_dict[k] = average\n",
    "    \n",
    "# df_dict_skewness = pd.read_table('skewness_dict.txt',delim_whitespace =True,header=None)\n",
    "df_dict_skewness = pd.Series(skewness_dict).to_frame('ColumnName').reset_index()\n",
    "df_dict_skewness.columns = { 'word',  'skewness'}\n",
    "df_dict_skewness.head()\n",
    "\n",
    "to_rmv = df_dict_skewness[df_dict_skewness.skewness<0.6].word.reset_index().word\n",
    "to_rmv.head()\n",
    "\n",
    "to_rmv_set = set(list(to_rmv)+list(to_remove))\n",
    "\n",
    "ngramlist3 = []\n",
    "# Removing the low skewness words\n",
    "final_train_doclist_eval = preprocess.remove_words_tuples_corpus(new_train_docs_eval,ngramlist3,to_rmv_set)\n",
    "\n",
    "final_test_doclist_eval = preprocess.remove_words_tuples_corpus(new_test_docs_eval,ngramlist3,to_rmv_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of token, 0\n",
      "index - 15311\n",
      "length of token, 0\n",
      "index - 70829\n",
      "total number  2\n"
     ]
    }
   ],
   "source": [
    "# removing empty training documents\n",
    "final_train_doclist1_eval = list(final_train_doclist_eval) \n",
    "less_than_10_idx_eval = preprocess.too_short_item(final_train_doclist_eval, 1,-1,1000000)\n",
    "final_train_doclist_eval = [final_train_doclist_eval[i] for i in range(len(final_train_doclist1_eval)) if i not in less_than_10_idx_eval]\n",
    "labels_eval = list(labels)\n",
    "labels1_eval = list(labels)\n",
    "train_labels_new_eval = [labels_eval[i] for i in range(len(labels1_eval)) if i not in less_than_10_idx_eval]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_doclist_concated_eval =  preprocess.finalizing_document_list(final_train_doclist_eval)\n",
    "test_doclist_concated_eval =  preprocess.finalizing_document_list(final_test_doclist_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving in txt file\n",
    "idx1 = np.random.permutation(len(training_doclist_concated_eval))\n",
    "x_train1 = [training_doclist_concated_eval[i] for i in idx1]\n",
    "y_train1 = [train_labels_new_eval[i] for i in idx1]\n",
    "sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=321)\n",
    "for train_index, test_index in sss.split(x_train1, y_train1):\n",
    "    X_train, X_test = [x_train1[ind] for ind in train_index ], [x_train1[ind] for ind in test_index ]\n",
    "    y_train, y_test = [y_train1[ind] for ind in train_index ], [y_train1[ind] for ind in test_index ]\n",
    "    break\n",
    "\n",
    "complete_doc = X_train+X_test\n",
    "\n",
    "       \n",
    "    \n",
    "with open('../Data/processed/complete_data_no_ngram1.txt','w+') as o_fh:\n",
    "    for doc in complete_doc:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()                \n",
    "   \n",
    "with open('../Data/processed/train_data_no_ngram1.txt','w+') as o_fh:\n",
    "    for doc in X_train:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()  \n",
    "\n",
    "\n",
    "with open('../Data/processed/validation_data_no_ngram1.txt','w+') as o_fh:\n",
    "    for doc in X_test:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()  \n",
    "\n",
    "label_df_train=pd.DataFrame(y_train,columns=['y'])\n",
    "label_df_validation=pd.DataFrame(y_test,columns=['y'])\n",
    "label_df_train.to_csv('../Data/processed/train_labels_no_ngram1.csv')\n",
    "label_df_validation.to_csv('../Data/processed/validation_labels_no_ngram1.csv')\n",
    "\n",
    "\n",
    "# Saving processed test data as well in form of a txt file\n",
    "with open('../Data/processed/test_data_no_ngram1.txt','w+') as o_fh:\n",
    "    for doc in test_doclist_concated_eval:\n",
    "        o_fh.write('{}'.format(doc))\n",
    "            \n",
    "        o_fh.write('\\n')\n",
    "o_fh.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "## News articles have now been preprocessed and cleaned and can be used for modelling in next task"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
